{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パスの追加\n",
    "import sys\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python36.zip')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/lib-dynload')\n",
    "sys.path.append('/home/siny/.local/lib/python3.6/site-packages')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 BERTの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT_Baseのネットワークの設定ファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 32006}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定をconfig.jsonから読み込み、JSONの辞書変数をオブジェクト変数に変換\n",
    "import json\n",
    "\n",
    "config_file = \"./weights/bert_config.json\"\n",
    "\n",
    "# ファイルを開き、JSONとして読み込む\n",
    "json_file = open(config_file, 'r')\n",
    "config = json.load(json_file)\n",
    "\n",
    "# 出力確認\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 辞書変数をオブジェクト変数に\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT用にLayerNormalization層を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT用にLayerNormalization層を定義します。\n",
    "# 実装の細かな点をTensorFlowに合わせています。\n",
    "\n",
    "\n",
    "class BertLayerNorm(nn.Module):\n",
    "    \"\"\"LayerNormalization層 \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))  # weightのこと\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))  # biasのこと\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingsモジュールの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTのEmbeddingsモジュールです\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"文章の単語ID列と、1文目か2文目かの情報を、埋め込みベクトルに変換する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        # （注釈）padding_idx=0はidx=0の単語のベクトルは0にする。BERTのボキャブラリーのidx=0が[PAD]である。\n",
    "\n",
    "        # Transformer Positional Embedding：位置情報テンソルをベクトルに変換\n",
    "        # Transformerの場合はsin、cosからなる固定値だったが、BERTは学習させる\n",
    "        # max_position_embeddings = 512　で文の長さは512単語\n",
    "        # Embedding(512, 768)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        # Sentence Embedding：文章の1文目、2文目の情報をベクトルに変換\n",
    "        # type_vocab_size = 2\n",
    "        # Embedding(2, 768)\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # 作成したLayerNormalization層\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # Dropout　'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids： [batch_size, seq_len]の文章の単語IDの羅列\n",
    "        token_type_ids：[batch_size, seq_len]の各単語が1文目なのか、2文目なのかを示すid\n",
    "        '''\n",
    "\n",
    "        # 1. Token Embeddings\n",
    "        # 単語IDを単語ベクトルに変換\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        # 2. Sentence Embedding\n",
    "        # token_type_idsがない場合は文章の全単語を1文目として、0にする\n",
    "        # そこで、input_idsと同じサイズのゼロテンソルを作成\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 3. Transformer Positional Embedding：\n",
    "        # [0, 1, 2 ・・・]と文章の長さだけ、数字が1つずつ昇順に入った\n",
    "        # [batch_size, seq_len]のテンソルposition_idsを作成\n",
    "        # position_idsを入力して、position_embeddings層から768次元のテンソルを取り出す\n",
    "        seq_length = input_ids.size(1)  # 文章の長さ\n",
    "        # seq_length分の1次元tensor([0, 1, 2, 3, 4, ・・・・])を生成\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        # unsqueeze(0)で#  torch.Size([5])->torch.Size([1, 5])のように変換\n",
    "        # expand_as(input_ids)で input_idsの要素数分に拡張する。　torch.size([1,5])をinput_ids分に変換＝torch.Size([2, 5])\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # 3つの埋め込みテンソルを足し合わせる [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # LayerNormalizationとDropoutを実行\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の単語ID列のテンソルサイズ： torch.Size([2, 5])\n",
      "入力の文章IDのテンソルサイズ： torch.Size([2, 5])\n",
      "seq_length= 5\n",
      "位置情報のテンソルサイズ= torch.Size([5])\n",
      "位置情報= tensor([0, 1, 2, 3, 4])\n",
      "BertEmbeddingsの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "tensor([[[ 0.4352, -0.2424,  0.1247,  ..., -0.3563, -1.1467,  0.3081],\n",
      "         [ 0.9814,  1.1865,  0.8787,  ...,  0.7517, -0.7765,  1.1848],\n",
      "         [ 0.2676, -0.3994,  1.6755,  ..., -0.6532,  1.9774,  1.0890],\n",
      "         [ 0.9233, -0.2370, -0.1749,  ...,  0.9287,  0.0000,  0.3507],\n",
      "         [ 0.0387, -0.1611,  1.0044,  ...,  0.0126, -0.2702, -0.0062]],\n",
      "\n",
      "        [[-0.0841,  0.3311,  0.0000,  ..., -0.9344, -0.6003,  0.2028],\n",
      "         [ 1.0774,  0.6626,  0.1831,  ...,  1.6676,  0.0893,  0.6564],\n",
      "         [ 0.3930, -0.0605,  0.0631,  ...,  0.5968,  0.2146,  0.0028],\n",
      "         [ 0.0980,  0.7774, -1.7313,  ..., -1.0900,  0.0206,  0.7337],\n",
      "         [ 0.4944, -0.8805,  1.2763,  ..., -0.1561, -0.0150,  0.5750]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#BERT Embeddingsクラスの形状チェック\n",
    "\n",
    "# 入力の単語ID列、batch_sizeは2つ\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
    "\n",
    "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
    "\n",
    "seq_length = input_ids.size(1)  # 文章の長さ\n",
    "print(\"seq_length=\", seq_length)\n",
    "# seq_length分の1次元tensor([0, 1, 2, 3, 4, ・・・・])を生成\n",
    "#位置情報の設定\n",
    "position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "print(\"位置情報のテンソルサイズ=\", position_ids.shape)\n",
    "print(\"位置情報=\", position_ids)\n",
    "# BERTの各モジュールを用意\n",
    "embeddings = BertEmbeddings(config)\n",
    "\n",
    "\n",
    "\n",
    "# 順伝搬する\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print(\"BertEmbeddingsの出力テンソル＝\", out1.shape)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertLayerモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERTのBertLayerモジュールです。Transformerになります'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "\n",
    "        # Self-Attention部分\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        # Self-Attentionの出力を処理する全結合層\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        # Self-Attentionによる特徴量とBertLayerへの元の入力を足し算する層\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embedderモジュールの出力テンソル[batch_size, seq_len, hidden_size]\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキング\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "            return layer_output  # [batch_size, seq_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    '''BertLayerモジュールのSelf-Attention部分です'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            #print(\"BertAttentionクラスの出力テンソルとattention_probsのテンソル\", attention_output.shape, attention_probs.shape)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            #print(\"BertAttentionクラスの出力テンソル＝\", attention_output.shape)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttentionのSelf-Attentionです'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads': 12\n",
    "\n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads)  # 768/12=64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size  # = 'hidden_size': 768\n",
    "\n",
    "        # Self-Attentionの特徴量を作成する全結合層\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        '''multi-head Attention用にテンソルの形を変換する\n",
    "        [batch_size, seq_len, hidden] → [batch_size, 12, seq_len, hidden/12] \n",
    "        '''\n",
    "        new_x_shape = x.size()[\n",
    "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        # 入力を全結合層で特徴量変換（注意、multi-head Attentionの全部をまとめて変換しています）\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        #print(\"BertSelfAttention:Qテンソル＝\", mixed_query_layer.shape)\n",
    "        #print(\"BertSelfAttention:Kテンソル＝\", mixed_key_layer.shape)\n",
    "        #print(\"BertSelfAttention:Vテンソル＝\", mixed_value_layer.shape)\n",
    "\n",
    "        # multi-head Attention用にテンソルの形を変換\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "        #print(\"BertSelfAttention:multi-head Attention用Qテンソル＝\", query_layer.shape)\n",
    "        #print(\"BertSelfAttention:multi-head Attention用Kテンソル＝\", key_layer.shape)\n",
    "        #print(\"BertSelfAttention:multi-head Attention用Vテンソル＝\", value_layer.shape)\n",
    "\n",
    "        # 特徴量同士を掛け算して似ている度合をAttention_scoresとして求める\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "        print(\"BertSelfAttention:attention_scoresテンソル＝\", attention_scores.shape)\n",
    "\n",
    "        # マスクがある部分にはマスクをかけます\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        #print(\"BertSelfAttention: attention_maskテンソル＝\", attention_mask.shape)\n",
    "        #print(\"BertSelfAttention: attention_scoresマスク追加後のテンソル＝\", attention_scores.shape)\n",
    "        # （備考）\n",
    "        # マスクが掛け算でなく足し算なのが直感的でないですが、このあとSoftmaxで正規化するので、\n",
    "        # マスクされた部分は-infにしたいです。 attention_maskには、0か-infが\n",
    "        # もともと入っているので足し算にしています。\n",
    "\n",
    "        # Attentionを正規化する\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        #print(\"BertSelfAttention: attention_probsSoftmax後のテンソル＝\", attention_probs.shape)\n",
    "\n",
    "        # ドロップアウトします\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Attention Mapを掛け算します\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        #print(\"BertSelfAttention: context_layerのテンソル＝\", context_layer.shape)\n",
    "        # multi-head Attentionのテンソルの形をもとに戻す\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[\n",
    "            :-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        print(\"BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝\", context_layer.shape)\n",
    "\n",
    "        # attention_showのときは、attention_probsもリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttentionの出力を処理する全結合層です'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states：BertSelfAttentionの出力テンソル\n",
    "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        #print(\"BertSelfOutputのhidden_statesテンソル＝\", hidden_states.shape)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        print(\"BertSelfOutputの出力テンソル＝\", hidden_states.shape)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unitという活性化関数です。\n",
    "    LeLUが0でカクっと不連続なので、そこを連続になるように滑らかにした形のLeLUです。\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 全結合層：'hidden_size': 768、'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        \n",
    "        # 活性化関数gelu\n",
    "        self.intermediate_act_fn = gelu\n",
    "            \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states： BertAttentionの出力テンソル\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)  # GELUによる活性化\n",
    "        print(\"BertIntermediateの出力テンソル＝\", hidden_states.shape)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "\n",
    "        # 全結合層：'intermediate_size': 3072、'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states： BertIntermediateの出力テンソル\n",
    "        input_tensor：BertAttentionの出力テンソル\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        print(\"BertOutput:hidden_statesのテンソル＝\", hidden_states.shape)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayerモジュールの繰り返し部分モジュールの繰り返し部分です\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayerモジュールの繰り返し部分モジュールの繰り返し部分です'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        # config.num_hidden_layers の値、すなわち12 個のBertLayerモジュールを作ります\n",
    "        self.layer = nn.ModuleList([BertLayer(config)\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embeddingsモジュールの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：返り値を全TransformerBlockモジュールの出力にするか、\n",
    "        それとも、最終層だけにするかのフラグ。\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # 返り値として使うリスト\n",
    "        all_encoder_layers = []\n",
    "        #print(\"BertEncoder:入力のhidden_statesテンソル＝\", hidden_states.shape)\n",
    "        #print(\"BertEncoder:入力のattention_maskテンソル＝\", attention_mask.shape)\n",
    "\n",
    "        # BertLayerモジュールの処理を繰り返す\n",
    "        for layer_module in self.layer:\n",
    "\n",
    "            if attention_show_flg == True:\n",
    "                '''attention_showのときは、attention_probsもリターンする'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "            elif attention_show_flg == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "                #print(\"BertEncoder:hidden_statesテンソル＝\",hidden_states.shape)\n",
    "\n",
    "            # 返り値にBertLayerから出力された特徴量を12層分、すべて使用する場合の処理\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "                #print(\"BertEncoder:all_encoder_layersテンソル!!!!＝\",len(all_encoder_layers))\n",
    "\n",
    "        # 返り値に最後のBertLayerから出力された特徴量だけを使う場合の処理\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "\n",
    "        # attention_showのときは、attention_probs（最後の12段目）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return all_encoder_layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertPoolerモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''入力文章の1単語目[cls]の特徴量を変換して保持するためのモジュール'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        # 全結合層、'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1単語目の特徴量を取得\n",
    "        #print(\"BertPooler:入力テンソル＝\", hidden_states.shape)\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        # 全結合層で特徴量変換\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        #print(\"BertPooler:pooled_outputテンソル＝\", pooled_output.shape)\n",
    "\n",
    "        # 活性化関数Tanhを計算\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        print(\"BertPooler:最終出力テンソル＝\", pooled_output.shape)\n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の単語ID列のテンソルサイズ： torch.Size([2, 5])\n",
      "入力のマスクのテンソルサイズ： torch.Size([2, 5])\n",
      "入力の文章IDのテンソルサイズ： torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# 入力の単語ID列、batch_sizeは2つ\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
    "\n",
    "# マスク\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print(\"入力のマスクのテンソルサイズ：\", attention_mask.shape)\n",
    "\n",
    "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
    "\n",
    "\n",
    "# BERTの各モジュールを用意\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拡張したマスクのテンソルサイズ： torch.Size([2, 1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# マスクの変形　[batch_size, 1, 1, seq_length]にする\n",
    "# Attentionをかけない部分はマイナス無限にしたいので、代わりに-10000をかけ算しています\n",
    "attention_mask   # (2,5)\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # ([2, 1, 1, 5]\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32) # [2, 1, 1, 5] to float\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0   # 1ではない要素を-10000（マイナス∞に置き換える）\n",
    "print(\"拡張したマスクのテンソルサイズ：\", extended_attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddingsの出力テンソルサイズ： torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertPooler:最終出力テンソル＝ torch.Size([2, 768])\n",
      "BertPoolerの出力テンソルサイズ： torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 順伝搬する\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print(\"BertEmbeddingsの出力テンソルサイズ：\", out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2は、[minibatch, seq_length, embedding_dim]が12個のリスト\n",
    "\n",
    "\n",
    "out3 = pooler(out2[-1])  # out2は12層の特徴量のリストになっているので一番最後を使用\n",
    "# BertPoolerの出力テンソルサイズ： torch.Size([2, 768])\n",
    "print(\"BertPoolerの出力テンソルサイズ：\", out3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部をつなげてBERTモデルにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''モジュールを全部つなげたBERTモデル'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        # 3つのモジュールを作成\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # Attentionのマスクと文の1文目、2文目のidが無ければ作成する\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # マスクの変形　[minibatch, 1, 1, seq_length]にする\n",
    "        # 後ほどmulti-head Attentionで使用できる形にしたいので\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # マスクは0、1だがソフトマックスを計算したときにマスクになるように、0と-infにする\n",
    "        # -infの代わりに-10000にしておく\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # 順伝搬させる\n",
    "        # BertEmbeddinsモジュール\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "\n",
    "        # BertLayerモジュール（Transformer）を繰り返すBertEncoderモジュール\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "\n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # BertPoolerモジュール\n",
    "        # encoderの一番最後のBertLayerから出力された特徴量を使う\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "\n",
    "        # output_all_encoded_layersがFalseの場合はリストではなく、テンソルを返す\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return encoded_layers, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([2, 12, 5, 5])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([2, 5, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([2, 5, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([2, 5, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([2, 5, 768])\n",
      "BertPooler:最終出力テンソル＝ torch.Size([2, 768])\n",
      "encoded_layersのテンソルサイズ： torch.Size([2, 5, 768])\n",
      "pooled_outputのテンソルサイズ： torch.Size([2, 768])\n",
      "attention_probsのテンソルサイズ： torch.Size([2, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "# 入力の用意\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERTモデルを作る\n",
    "net = BertModel(config)\n",
    "\n",
    "# 順伝搬させる\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "print(\"encoded_layersのテンソルサイズ：\", encoded_layers.shape)\n",
    "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
    "print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 BERTを用いたbank（銀行）とbank（土手）の単語ベクトル表現の比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.seq_relationship.weight\n",
      "cls.seq_relationship.bias\n"
     ]
    }
   ],
   "source": [
    "# 学習済みモデルのロード\n",
    "weights_path = \"./weights/pytorch_model.bin\"\n",
    "loaded_state_dict = torch.load(weights_path)\n",
    "\n",
    "for s in loaded_state_dict.keys():\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.gamma\n",
      "embeddings.LayerNorm.beta\n",
      "encoder.layer.0.attention.selfattn.query.weight\n",
      "encoder.layer.0.attention.selfattn.query.bias\n",
      "encoder.layer.0.attention.selfattn.key.weight\n",
      "encoder.layer.0.attention.selfattn.key.bias\n",
      "encoder.layer.0.attention.selfattn.value.weight\n",
      "encoder.layer.0.attention.selfattn.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "encoder.layer.0.attention.output.LayerNorm.beta\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.gamma\n",
      "encoder.layer.0.output.LayerNorm.beta\n",
      "encoder.layer.1.attention.selfattn.query.weight\n",
      "encoder.layer.1.attention.selfattn.query.bias\n",
      "encoder.layer.1.attention.selfattn.key.weight\n",
      "encoder.layer.1.attention.selfattn.key.bias\n",
      "encoder.layer.1.attention.selfattn.value.weight\n",
      "encoder.layer.1.attention.selfattn.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "encoder.layer.1.attention.output.LayerNorm.beta\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.gamma\n",
      "encoder.layer.1.output.LayerNorm.beta\n",
      "encoder.layer.2.attention.selfattn.query.weight\n",
      "encoder.layer.2.attention.selfattn.query.bias\n",
      "encoder.layer.2.attention.selfattn.key.weight\n",
      "encoder.layer.2.attention.selfattn.key.bias\n",
      "encoder.layer.2.attention.selfattn.value.weight\n",
      "encoder.layer.2.attention.selfattn.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "encoder.layer.2.attention.output.LayerNorm.beta\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.gamma\n",
      "encoder.layer.2.output.LayerNorm.beta\n",
      "encoder.layer.3.attention.selfattn.query.weight\n",
      "encoder.layer.3.attention.selfattn.query.bias\n",
      "encoder.layer.3.attention.selfattn.key.weight\n",
      "encoder.layer.3.attention.selfattn.key.bias\n",
      "encoder.layer.3.attention.selfattn.value.weight\n",
      "encoder.layer.3.attention.selfattn.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "encoder.layer.3.attention.output.LayerNorm.beta\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.gamma\n",
      "encoder.layer.3.output.LayerNorm.beta\n",
      "encoder.layer.4.attention.selfattn.query.weight\n",
      "encoder.layer.4.attention.selfattn.query.bias\n",
      "encoder.layer.4.attention.selfattn.key.weight\n",
      "encoder.layer.4.attention.selfattn.key.bias\n",
      "encoder.layer.4.attention.selfattn.value.weight\n",
      "encoder.layer.4.attention.selfattn.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "encoder.layer.4.attention.output.LayerNorm.beta\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.gamma\n",
      "encoder.layer.4.output.LayerNorm.beta\n",
      "encoder.layer.5.attention.selfattn.query.weight\n",
      "encoder.layer.5.attention.selfattn.query.bias\n",
      "encoder.layer.5.attention.selfattn.key.weight\n",
      "encoder.layer.5.attention.selfattn.key.bias\n",
      "encoder.layer.5.attention.selfattn.value.weight\n",
      "encoder.layer.5.attention.selfattn.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "encoder.layer.5.attention.output.LayerNorm.beta\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.gamma\n",
      "encoder.layer.5.output.LayerNorm.beta\n",
      "encoder.layer.6.attention.selfattn.query.weight\n",
      "encoder.layer.6.attention.selfattn.query.bias\n",
      "encoder.layer.6.attention.selfattn.key.weight\n",
      "encoder.layer.6.attention.selfattn.key.bias\n",
      "encoder.layer.6.attention.selfattn.value.weight\n",
      "encoder.layer.6.attention.selfattn.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "encoder.layer.6.attention.output.LayerNorm.beta\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.gamma\n",
      "encoder.layer.6.output.LayerNorm.beta\n",
      "encoder.layer.7.attention.selfattn.query.weight\n",
      "encoder.layer.7.attention.selfattn.query.bias\n",
      "encoder.layer.7.attention.selfattn.key.weight\n",
      "encoder.layer.7.attention.selfattn.key.bias\n",
      "encoder.layer.7.attention.selfattn.value.weight\n",
      "encoder.layer.7.attention.selfattn.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "encoder.layer.7.attention.output.LayerNorm.beta\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.gamma\n",
      "encoder.layer.7.output.LayerNorm.beta\n",
      "encoder.layer.8.attention.selfattn.query.weight\n",
      "encoder.layer.8.attention.selfattn.query.bias\n",
      "encoder.layer.8.attention.selfattn.key.weight\n",
      "encoder.layer.8.attention.selfattn.key.bias\n",
      "encoder.layer.8.attention.selfattn.value.weight\n",
      "encoder.layer.8.attention.selfattn.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "encoder.layer.8.attention.output.LayerNorm.beta\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.gamma\n",
      "encoder.layer.8.output.LayerNorm.beta\n",
      "encoder.layer.9.attention.selfattn.query.weight\n",
      "encoder.layer.9.attention.selfattn.query.bias\n",
      "encoder.layer.9.attention.selfattn.key.weight\n",
      "encoder.layer.9.attention.selfattn.key.bias\n",
      "encoder.layer.9.attention.selfattn.value.weight\n",
      "encoder.layer.9.attention.selfattn.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "encoder.layer.9.attention.output.LayerNorm.beta\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.gamma\n",
      "encoder.layer.9.output.LayerNorm.beta\n",
      "encoder.layer.10.attention.selfattn.query.weight\n",
      "encoder.layer.10.attention.selfattn.query.bias\n",
      "encoder.layer.10.attention.selfattn.key.weight\n",
      "encoder.layer.10.attention.selfattn.key.bias\n",
      "encoder.layer.10.attention.selfattn.value.weight\n",
      "encoder.layer.10.attention.selfattn.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "encoder.layer.10.attention.output.LayerNorm.beta\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.gamma\n",
      "encoder.layer.10.output.LayerNorm.beta\n",
      "encoder.layer.11.attention.selfattn.query.weight\n",
      "encoder.layer.11.attention.selfattn.query.bias\n",
      "encoder.layer.11.attention.selfattn.key.weight\n",
      "encoder.layer.11.attention.selfattn.key.bias\n",
      "encoder.layer.11.attention.selfattn.value.weight\n",
      "encoder.layer.11.attention.selfattn.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "encoder.layer.11.attention.output.LayerNorm.beta\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.gamma\n",
      "encoder.layer.11.output.LayerNorm.beta\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.LayerNorm.gamma',\n",
       " 'embeddings.LayerNorm.beta',\n",
       " 'encoder.layer.0.attention.selfattn.query.weight',\n",
       " 'encoder.layer.0.attention.selfattn.query.bias',\n",
       " 'encoder.layer.0.attention.selfattn.key.weight',\n",
       " 'encoder.layer.0.attention.selfattn.key.bias',\n",
       " 'encoder.layer.0.attention.selfattn.value.weight',\n",
       " 'encoder.layer.0.attention.selfattn.value.bias',\n",
       " 'encoder.layer.0.attention.output.dense.weight',\n",
       " 'encoder.layer.0.attention.output.dense.bias',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.0.intermediate.dense.weight',\n",
       " 'encoder.layer.0.intermediate.dense.bias',\n",
       " 'encoder.layer.0.output.dense.weight',\n",
       " 'encoder.layer.0.output.dense.bias',\n",
       " 'encoder.layer.0.output.LayerNorm.gamma',\n",
       " 'encoder.layer.0.output.LayerNorm.beta',\n",
       " 'encoder.layer.1.attention.selfattn.query.weight',\n",
       " 'encoder.layer.1.attention.selfattn.query.bias',\n",
       " 'encoder.layer.1.attention.selfattn.key.weight',\n",
       " 'encoder.layer.1.attention.selfattn.key.bias',\n",
       " 'encoder.layer.1.attention.selfattn.value.weight',\n",
       " 'encoder.layer.1.attention.selfattn.value.bias',\n",
       " 'encoder.layer.1.attention.output.dense.weight',\n",
       " 'encoder.layer.1.attention.output.dense.bias',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.1.intermediate.dense.weight',\n",
       " 'encoder.layer.1.intermediate.dense.bias',\n",
       " 'encoder.layer.1.output.dense.weight',\n",
       " 'encoder.layer.1.output.dense.bias',\n",
       " 'encoder.layer.1.output.LayerNorm.gamma',\n",
       " 'encoder.layer.1.output.LayerNorm.beta',\n",
       " 'encoder.layer.2.attention.selfattn.query.weight',\n",
       " 'encoder.layer.2.attention.selfattn.query.bias',\n",
       " 'encoder.layer.2.attention.selfattn.key.weight',\n",
       " 'encoder.layer.2.attention.selfattn.key.bias',\n",
       " 'encoder.layer.2.attention.selfattn.value.weight',\n",
       " 'encoder.layer.2.attention.selfattn.value.bias',\n",
       " 'encoder.layer.2.attention.output.dense.weight',\n",
       " 'encoder.layer.2.attention.output.dense.bias',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.2.intermediate.dense.weight',\n",
       " 'encoder.layer.2.intermediate.dense.bias',\n",
       " 'encoder.layer.2.output.dense.weight',\n",
       " 'encoder.layer.2.output.dense.bias',\n",
       " 'encoder.layer.2.output.LayerNorm.gamma',\n",
       " 'encoder.layer.2.output.LayerNorm.beta',\n",
       " 'encoder.layer.3.attention.selfattn.query.weight',\n",
       " 'encoder.layer.3.attention.selfattn.query.bias',\n",
       " 'encoder.layer.3.attention.selfattn.key.weight',\n",
       " 'encoder.layer.3.attention.selfattn.key.bias',\n",
       " 'encoder.layer.3.attention.selfattn.value.weight',\n",
       " 'encoder.layer.3.attention.selfattn.value.bias',\n",
       " 'encoder.layer.3.attention.output.dense.weight',\n",
       " 'encoder.layer.3.attention.output.dense.bias',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.3.intermediate.dense.weight',\n",
       " 'encoder.layer.3.intermediate.dense.bias',\n",
       " 'encoder.layer.3.output.dense.weight',\n",
       " 'encoder.layer.3.output.dense.bias',\n",
       " 'encoder.layer.3.output.LayerNorm.gamma',\n",
       " 'encoder.layer.3.output.LayerNorm.beta',\n",
       " 'encoder.layer.4.attention.selfattn.query.weight',\n",
       " 'encoder.layer.4.attention.selfattn.query.bias',\n",
       " 'encoder.layer.4.attention.selfattn.key.weight',\n",
       " 'encoder.layer.4.attention.selfattn.key.bias',\n",
       " 'encoder.layer.4.attention.selfattn.value.weight',\n",
       " 'encoder.layer.4.attention.selfattn.value.bias',\n",
       " 'encoder.layer.4.attention.output.dense.weight',\n",
       " 'encoder.layer.4.attention.output.dense.bias',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.4.intermediate.dense.weight',\n",
       " 'encoder.layer.4.intermediate.dense.bias',\n",
       " 'encoder.layer.4.output.dense.weight',\n",
       " 'encoder.layer.4.output.dense.bias',\n",
       " 'encoder.layer.4.output.LayerNorm.gamma',\n",
       " 'encoder.layer.4.output.LayerNorm.beta',\n",
       " 'encoder.layer.5.attention.selfattn.query.weight',\n",
       " 'encoder.layer.5.attention.selfattn.query.bias',\n",
       " 'encoder.layer.5.attention.selfattn.key.weight',\n",
       " 'encoder.layer.5.attention.selfattn.key.bias',\n",
       " 'encoder.layer.5.attention.selfattn.value.weight',\n",
       " 'encoder.layer.5.attention.selfattn.value.bias',\n",
       " 'encoder.layer.5.attention.output.dense.weight',\n",
       " 'encoder.layer.5.attention.output.dense.bias',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.5.intermediate.dense.weight',\n",
       " 'encoder.layer.5.intermediate.dense.bias',\n",
       " 'encoder.layer.5.output.dense.weight',\n",
       " 'encoder.layer.5.output.dense.bias',\n",
       " 'encoder.layer.5.output.LayerNorm.gamma',\n",
       " 'encoder.layer.5.output.LayerNorm.beta',\n",
       " 'encoder.layer.6.attention.selfattn.query.weight',\n",
       " 'encoder.layer.6.attention.selfattn.query.bias',\n",
       " 'encoder.layer.6.attention.selfattn.key.weight',\n",
       " 'encoder.layer.6.attention.selfattn.key.bias',\n",
       " 'encoder.layer.6.attention.selfattn.value.weight',\n",
       " 'encoder.layer.6.attention.selfattn.value.bias',\n",
       " 'encoder.layer.6.attention.output.dense.weight',\n",
       " 'encoder.layer.6.attention.output.dense.bias',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.6.intermediate.dense.weight',\n",
       " 'encoder.layer.6.intermediate.dense.bias',\n",
       " 'encoder.layer.6.output.dense.weight',\n",
       " 'encoder.layer.6.output.dense.bias',\n",
       " 'encoder.layer.6.output.LayerNorm.gamma',\n",
       " 'encoder.layer.6.output.LayerNorm.beta',\n",
       " 'encoder.layer.7.attention.selfattn.query.weight',\n",
       " 'encoder.layer.7.attention.selfattn.query.bias',\n",
       " 'encoder.layer.7.attention.selfattn.key.weight',\n",
       " 'encoder.layer.7.attention.selfattn.key.bias',\n",
       " 'encoder.layer.7.attention.selfattn.value.weight',\n",
       " 'encoder.layer.7.attention.selfattn.value.bias',\n",
       " 'encoder.layer.7.attention.output.dense.weight',\n",
       " 'encoder.layer.7.attention.output.dense.bias',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.7.intermediate.dense.weight',\n",
       " 'encoder.layer.7.intermediate.dense.bias',\n",
       " 'encoder.layer.7.output.dense.weight',\n",
       " 'encoder.layer.7.output.dense.bias',\n",
       " 'encoder.layer.7.output.LayerNorm.gamma',\n",
       " 'encoder.layer.7.output.LayerNorm.beta',\n",
       " 'encoder.layer.8.attention.selfattn.query.weight',\n",
       " 'encoder.layer.8.attention.selfattn.query.bias',\n",
       " 'encoder.layer.8.attention.selfattn.key.weight',\n",
       " 'encoder.layer.8.attention.selfattn.key.bias',\n",
       " 'encoder.layer.8.attention.selfattn.value.weight',\n",
       " 'encoder.layer.8.attention.selfattn.value.bias',\n",
       " 'encoder.layer.8.attention.output.dense.weight',\n",
       " 'encoder.layer.8.attention.output.dense.bias',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.8.intermediate.dense.weight',\n",
       " 'encoder.layer.8.intermediate.dense.bias',\n",
       " 'encoder.layer.8.output.dense.weight',\n",
       " 'encoder.layer.8.output.dense.bias',\n",
       " 'encoder.layer.8.output.LayerNorm.gamma',\n",
       " 'encoder.layer.8.output.LayerNorm.beta',\n",
       " 'encoder.layer.9.attention.selfattn.query.weight',\n",
       " 'encoder.layer.9.attention.selfattn.query.bias',\n",
       " 'encoder.layer.9.attention.selfattn.key.weight',\n",
       " 'encoder.layer.9.attention.selfattn.key.bias',\n",
       " 'encoder.layer.9.attention.selfattn.value.weight',\n",
       " 'encoder.layer.9.attention.selfattn.value.bias',\n",
       " 'encoder.layer.9.attention.output.dense.weight',\n",
       " 'encoder.layer.9.attention.output.dense.bias',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.9.intermediate.dense.weight',\n",
       " 'encoder.layer.9.intermediate.dense.bias',\n",
       " 'encoder.layer.9.output.dense.weight',\n",
       " 'encoder.layer.9.output.dense.bias',\n",
       " 'encoder.layer.9.output.LayerNorm.gamma',\n",
       " 'encoder.layer.9.output.LayerNorm.beta',\n",
       " 'encoder.layer.10.attention.selfattn.query.weight',\n",
       " 'encoder.layer.10.attention.selfattn.query.bias',\n",
       " 'encoder.layer.10.attention.selfattn.key.weight',\n",
       " 'encoder.layer.10.attention.selfattn.key.bias',\n",
       " 'encoder.layer.10.attention.selfattn.value.weight',\n",
       " 'encoder.layer.10.attention.selfattn.value.bias',\n",
       " 'encoder.layer.10.attention.output.dense.weight',\n",
       " 'encoder.layer.10.attention.output.dense.bias',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.10.intermediate.dense.weight',\n",
       " 'encoder.layer.10.intermediate.dense.bias',\n",
       " 'encoder.layer.10.output.dense.weight',\n",
       " 'encoder.layer.10.output.dense.bias',\n",
       " 'encoder.layer.10.output.LayerNorm.gamma',\n",
       " 'encoder.layer.10.output.LayerNorm.beta',\n",
       " 'encoder.layer.11.attention.selfattn.query.weight',\n",
       " 'encoder.layer.11.attention.selfattn.query.bias',\n",
       " 'encoder.layer.11.attention.selfattn.key.weight',\n",
       " 'encoder.layer.11.attention.selfattn.key.bias',\n",
       " 'encoder.layer.11.attention.selfattn.value.weight',\n",
       " 'encoder.layer.11.attention.selfattn.value.bias',\n",
       " 'encoder.layer.11.attention.output.dense.weight',\n",
       " 'encoder.layer.11.attention.output.dense.bias',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.gamma',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.beta',\n",
       " 'encoder.layer.11.intermediate.dense.weight',\n",
       " 'encoder.layer.11.intermediate.dense.bias',\n",
       " 'encoder.layer.11.output.dense.weight',\n",
       " 'encoder.layer.11.output.dense.bias',\n",
       " 'encoder.layer.11.output.LayerNorm.gamma',\n",
       " 'encoder.layer.11.output.LayerNorm.beta',\n",
       " 'pooler.dense.weight',\n",
       " 'pooler.dense.bias']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの用意\n",
    "net = BertModel(config)\n",
    "net.eval()\n",
    "\n",
    "# 現在のネットワークモデルのパラメータ名\n",
    "param_names = []  # パラメータの名前を格納していく\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    param_names.append(name)\n",
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "書籍のモデル： torch.Size([32006, 768])\n",
      "学習済みモデル: torch.Size([32006, 768])\n"
     ]
    }
   ],
   "source": [
    "new_state_dict = net.state_dict().copy()\n",
    "print(\"書籍のモデル：\",new_state_dict['embeddings.word_embeddings.weight'].shape)\n",
    "print(\"学習済みモデル:\",loaded_state_dict['bert.embeddings.word_embeddings.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight→embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight→embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight→embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight→embeddings.LayerNorm.gamma\n",
      "bert.embeddings.LayerNorm.bias→embeddings.LayerNorm.beta\n",
      "bert.encoder.layer.0.attention.self.query.weight→encoder.layer.0.attention.selfattn.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias→encoder.layer.0.attention.selfattn.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight→encoder.layer.0.attention.selfattn.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias→encoder.layer.0.attention.selfattn.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight→encoder.layer.0.attention.selfattn.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias→encoder.layer.0.attention.selfattn.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight→encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias→encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight→encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias→encoder.layer.0.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.0.intermediate.dense.weight→encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias→encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight→encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias→encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight→encoder.layer.0.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.output.LayerNorm.bias→encoder.layer.0.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.attention.self.query.weight→encoder.layer.1.attention.selfattn.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias→encoder.layer.1.attention.selfattn.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight→encoder.layer.1.attention.selfattn.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias→encoder.layer.1.attention.selfattn.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight→encoder.layer.1.attention.selfattn.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias→encoder.layer.1.attention.selfattn.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight→encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias→encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight→encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias→encoder.layer.1.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.intermediate.dense.weight→encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias→encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight→encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias→encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight→encoder.layer.1.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.output.LayerNorm.bias→encoder.layer.1.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.attention.self.query.weight→encoder.layer.2.attention.selfattn.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias→encoder.layer.2.attention.selfattn.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight→encoder.layer.2.attention.selfattn.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias→encoder.layer.2.attention.selfattn.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight→encoder.layer.2.attention.selfattn.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias→encoder.layer.2.attention.selfattn.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight→encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias→encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight→encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias→encoder.layer.2.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.intermediate.dense.weight→encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias→encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight→encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias→encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight→encoder.layer.2.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.output.LayerNorm.bias→encoder.layer.2.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.attention.self.query.weight→encoder.layer.3.attention.selfattn.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias→encoder.layer.3.attention.selfattn.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight→encoder.layer.3.attention.selfattn.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias→encoder.layer.3.attention.selfattn.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight→encoder.layer.3.attention.selfattn.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias→encoder.layer.3.attention.selfattn.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight→encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias→encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight→encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias→encoder.layer.3.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.intermediate.dense.weight→encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias→encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight→encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias→encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight→encoder.layer.3.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.output.LayerNorm.bias→encoder.layer.3.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.attention.self.query.weight→encoder.layer.4.attention.selfattn.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias→encoder.layer.4.attention.selfattn.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight→encoder.layer.4.attention.selfattn.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias→encoder.layer.4.attention.selfattn.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight→encoder.layer.4.attention.selfattn.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias→encoder.layer.4.attention.selfattn.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight→encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias→encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight→encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias→encoder.layer.4.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.intermediate.dense.weight→encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias→encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight→encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias→encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight→encoder.layer.4.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.output.LayerNorm.bias→encoder.layer.4.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.attention.self.query.weight→encoder.layer.5.attention.selfattn.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias→encoder.layer.5.attention.selfattn.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight→encoder.layer.5.attention.selfattn.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias→encoder.layer.5.attention.selfattn.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight→encoder.layer.5.attention.selfattn.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias→encoder.layer.5.attention.selfattn.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight→encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias→encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight→encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias→encoder.layer.5.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.intermediate.dense.weight→encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias→encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight→encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias→encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight→encoder.layer.5.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.output.LayerNorm.bias→encoder.layer.5.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.attention.self.query.weight→encoder.layer.6.attention.selfattn.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias→encoder.layer.6.attention.selfattn.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight→encoder.layer.6.attention.selfattn.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias→encoder.layer.6.attention.selfattn.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight→encoder.layer.6.attention.selfattn.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias→encoder.layer.6.attention.selfattn.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight→encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias→encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight→encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias→encoder.layer.6.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.intermediate.dense.weight→encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias→encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight→encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias→encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight→encoder.layer.6.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.output.LayerNorm.bias→encoder.layer.6.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.attention.self.query.weight→encoder.layer.7.attention.selfattn.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias→encoder.layer.7.attention.selfattn.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight→encoder.layer.7.attention.selfattn.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias→encoder.layer.7.attention.selfattn.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight→encoder.layer.7.attention.selfattn.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias→encoder.layer.7.attention.selfattn.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight→encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias→encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight→encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias→encoder.layer.7.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.intermediate.dense.weight→encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias→encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight→encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias→encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight→encoder.layer.7.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.output.LayerNorm.bias→encoder.layer.7.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.attention.self.query.weight→encoder.layer.8.attention.selfattn.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias→encoder.layer.8.attention.selfattn.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight→encoder.layer.8.attention.selfattn.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias→encoder.layer.8.attention.selfattn.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight→encoder.layer.8.attention.selfattn.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias→encoder.layer.8.attention.selfattn.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight→encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias→encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight→encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias→encoder.layer.8.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.intermediate.dense.weight→encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias→encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight→encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias→encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight→encoder.layer.8.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.output.LayerNorm.bias→encoder.layer.8.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.attention.self.query.weight→encoder.layer.9.attention.selfattn.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias→encoder.layer.9.attention.selfattn.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight→encoder.layer.9.attention.selfattn.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias→encoder.layer.9.attention.selfattn.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight→encoder.layer.9.attention.selfattn.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias→encoder.layer.9.attention.selfattn.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight→encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias→encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight→encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias→encoder.layer.9.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.intermediate.dense.weight→encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias→encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight→encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias→encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight→encoder.layer.9.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.output.LayerNorm.bias→encoder.layer.9.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.attention.self.query.weight→encoder.layer.10.attention.selfattn.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias→encoder.layer.10.attention.selfattn.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight→encoder.layer.10.attention.selfattn.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias→encoder.layer.10.attention.selfattn.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight→encoder.layer.10.attention.selfattn.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias→encoder.layer.10.attention.selfattn.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight→encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias→encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight→encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias→encoder.layer.10.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.intermediate.dense.weight→encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias→encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight→encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias→encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight→encoder.layer.10.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.output.LayerNorm.bias→encoder.layer.10.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.attention.self.query.weight→encoder.layer.11.attention.selfattn.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias→encoder.layer.11.attention.selfattn.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight→encoder.layer.11.attention.selfattn.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias→encoder.layer.11.attention.selfattn.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight→encoder.layer.11.attention.selfattn.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias→encoder.layer.11.attention.selfattn.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight→encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias→encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight→encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias→encoder.layer.11.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.intermediate.dense.weight→encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias→encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight→encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias→encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight→encoder.layer.11.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.output.LayerNorm.bias→encoder.layer.11.output.LayerNorm.beta\n",
      "bert.pooler.dense.weight→pooler.dense.weight\n",
      "bert.pooler.dense.bias→pooler.dense.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dictの名前が違うので前から順番に代入する\n",
    "# 今回、パラメータの名前は違っていても、対応するものは同じ順番になっています\n",
    "\n",
    "# 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
    "new_state_dict = net.state_dict().copy()\n",
    "\n",
    "# 新たなstate_dictに学習済みの値を代入\n",
    "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
    "    name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
    "    new_state_dict[name] = value  # 値を入れる\n",
    "    print(str(key_name)+\"→\"+str(name))  # 何から何に入ったかを表示\n",
    "\n",
    "    # 現在のネットワークのパラメータを全部ロードしたら終える\n",
    "    if index+1 >= len(param_names):\n",
    "        break\n",
    "\n",
    "# 新たなstate_dictを実装したBERTモデルに与える\n",
    "net.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juman++による形態素解析の動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['魅力', 'が', 'たっぷり', 'と', '詰まって', 'いる']\n"
     ]
    }
   ],
   "source": [
    "from pyknp import Juman\n",
    "\n",
    "text = \"魅力がたっぷりと詰まっている\"\n",
    "juman = Juman()\n",
    "result =juman.analysis(text)\n",
    "print([mrph.midasi for mrph in result.mrph_list()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT用のTokenizerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabファイルを読み込み、\n",
    "import collections\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"text形式のvocabファイルの内容を辞書に格納します\"\"\"\n",
    "    vocab = collections.OrderedDict()  # (単語, id)の順番の辞書変数\n",
    "    ids_to_tokens = collections.OrderedDict()  # (id, 単語)の順番の辞書変数\n",
    "    index = 0\n",
    "\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "\n",
    "            # 格納\n",
    "            vocab[token] = index\n",
    "            ids_to_tokens[index] = token\n",
    "            index += 1\n",
    "\n",
    "    return vocab, ids_to_tokens\n",
    "\n",
    "\n",
    "# 実行\n",
    "vocab_file = \"./vocab/vocab.txt\"\n",
    "vocab, ids_to_tokens = load_vocab(vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[UNK]', 1),\n",
       "             ('[CLS]', 2),\n",
       "             ('[SEP]', 3),\n",
       "             ('[MASK]', 4),\n",
       "             ('の', 5),\n",
       "             ('、', 6),\n",
       "             ('。', 7),\n",
       "             ('に', 8),\n",
       "             ('は', 9),\n",
       "             ('を', 10),\n",
       "             ('が', 11),\n",
       "             ('と', 12),\n",
       "             ('で', 13),\n",
       "             ('年', 14),\n",
       "             ('・', 15),\n",
       "             ('（', 16),\n",
       "             ('）', 17),\n",
       "             ('さ', 18),\n",
       "             ('して', 19),\n",
       "             ('した', 20),\n",
       "             ('いる', 21),\n",
       "             ('する', 22),\n",
       "             ('も', 23),\n",
       "             ('「', 24),\n",
       "             ('」', 25),\n",
       "             ('月', 26),\n",
       "             ('から', 27),\n",
       "             ('れた', 28),\n",
       "             ('日', 29),\n",
       "             ('こと', 30),\n",
       "             ('し', 31),\n",
       "             ('である', 32),\n",
       "             ('れて', 33),\n",
       "             ('や', 34),\n",
       "             ('２', 35),\n",
       "             ('１', 36),\n",
       "             ('いた', 37),\n",
       "             ('ある', 38),\n",
       "             ('『', 39),\n",
       "             ('』', 40),\n",
       "             ('れる', 41),\n",
       "             ('など', 42),\n",
       "             ('３', 43),\n",
       "             ('−', 44),\n",
       "             ('', 2450),\n",
       "             ('この', 46),\n",
       "             ('ない', 47),\n",
       "             ('ため', 48),\n",
       "             ('日本', 49),\n",
       "             ('人', 50),\n",
       "             ('”', 51),\n",
       "             ('より', 52),\n",
       "             ('４', 53),\n",
       "             ('れ', 54),\n",
       "             ('第', 55),\n",
       "             ('いう', 56),\n",
       "             ('者', 57),\n",
       "             ('その', 58),\n",
       "             ('なった', 59),\n",
       "             ('もの', 60),\n",
       "             ('へ', 61),\n",
       "             ('後', 62),\n",
       "             ('まで', 63),\n",
       "             ('また', 64),\n",
       "             ('市', 65),\n",
       "             ('なる', 66),\n",
       "             ('５', 67),\n",
       "             ('中', 68),\n",
       "             ('６', 69),\n",
       "             ('一', 70),\n",
       "             ('同', 71),\n",
       "             ('県', 72),\n",
       "             ('これ', 73),\n",
       "             ('１０', 74),\n",
       "             ('７', 75),\n",
       "             ('内', 76),\n",
       "             ('８', 77),\n",
       "             ('なって', 78),\n",
       "             ('おり', 79),\n",
       "             ('よる', 80),\n",
       "             ('９', 81),\n",
       "             ('大学', 82),\n",
       "             ('つ', 83),\n",
       "             ('大', 84),\n",
       "             ('国', 85),\n",
       "             ('よって', 86),\n",
       "             ('時', 87),\n",
       "             ('１２', 88),\n",
       "             ('であった', 89),\n",
       "             ('か', 90),\n",
       "             ('家', 91),\n",
       "             ('駅', 92),\n",
       "             ('ように', 93),\n",
       "             ('ら', 94),\n",
       "             ('現在', 95),\n",
       "             ('的な', 96),\n",
       "             ('本', 97),\n",
       "             ('１１', 98),\n",
       "             ('軍', 99),\n",
       "             ('上', 100),\n",
       "             ('：', 101),\n",
       "             ('化', 102),\n",
       "             ('であり', 103),\n",
       "             ('的に', 104),\n",
       "             ('なり', 105),\n",
       "             ('放送', 106),\n",
       "             ('名', 107),\n",
       "             ('性', 108),\n",
       "             ('ず', 109),\n",
       "             ('部', 110),\n",
       "             ('回', 111),\n",
       "             ('目', 112),\n",
       "             ('町', 113),\n",
       "             ('時代', 114),\n",
       "             ('それ', 115),\n",
       "             ('なかった', 116),\n",
       "             ('おいて', 117),\n",
       "             ('世界', 118),\n",
       "             ('代', 119),\n",
       "             ('線', 120),\n",
       "             ('間', 121),\n",
       "             ('戦', 122),\n",
       "             ('でも', 123),\n",
       "             ('られる', 124),\n",
       "             ('あり', 125),\n",
       "             ('会', 126),\n",
       "             ('場合', 127),\n",
       "             ('行わ', 128),\n",
       "             ('二', 129),\n",
       "             ('ついて', 130),\n",
       "             ('所', 131),\n",
       "             ('その後', 132),\n",
       "             ('東京', 133),\n",
       "             ('前', 134),\n",
       "             ('多く', 135),\n",
       "             ('州', 136),\n",
       "             ('だった', 137),\n",
       "             ('地', 138),\n",
       "             ('あった', 139),\n",
       "             ('なく', 140),\n",
       "             ('しかし', 141),\n",
       "             ('い', 142),\n",
       "             ('られた', 143),\n",
       "             ('号', 144),\n",
       "             ('数', 145),\n",
       "             ('できる', 146),\n",
       "             ('的', 147),\n",
       "             ('作品', 148),\n",
       "             ('彼', 149),\n",
       "             ('選手', 150),\n",
       "             ('他', 151),\n",
       "             ('｜', 152),\n",
       "             ('使用', 153),\n",
       "             ('機', 154),\n",
       "             ('昭和', 155),\n",
       "             ('語', 156),\n",
       "             ('られて', 157),\n",
       "             ('郡', 158),\n",
       "             ('位', 159),\n",
       "             ('研究', 160),\n",
       "             ('当時', 161),\n",
       "             ('存在', 162),\n",
       "             ('新', 163),\n",
       "             ('元', 164),\n",
       "             ('アメリカ', 165),\n",
       "             ('長', 166),\n",
       "             ('側', 167),\n",
       "             ('三', 168),\n",
       "             ('活動', 169),\n",
       "             ('映画', 170),\n",
       "             ('初', 171),\n",
       "             ('学校', 172),\n",
       "             ('社', 173),\n",
       "             ('等', 174),\n",
       "             ('１５', 175),\n",
       "             ('，', 176),\n",
       "             ('全', 177),\n",
       "             ('下', 178),\n",
       "             ('番組', 179),\n",
       "             ('呼ば', 180),\n",
       "             ('東', 181),\n",
       "             ('区', 182),\n",
       "             ('２０', 183),\n",
       "             ('会社', 184),\n",
       "             ('出身', 185),\n",
       "             ('および', 186),\n",
       "             ('／', 187),\n",
       "             ('車', 188),\n",
       "             ('約', 189),\n",
       "             ('のみ', 190),\n",
       "             ('代表', 191),\n",
       "             ('形', 192),\n",
       "             ('権', 193),\n",
       "             ('なお', 194),\n",
       "             ('テレビ', 195),\n",
       "             ('西', 196),\n",
       "             ('系', 197),\n",
       "             ('発売', 198),\n",
       "             ('型', 199),\n",
       "             ('以下', 200),\n",
       "             ('地域', 201),\n",
       "             ('法', 202),\n",
       "             ('開発', 203),\n",
       "             ('１４', 204),\n",
       "             ('歳', 205),\n",
       "             ('作', 206),\n",
       "             ('１３', 207),\n",
       "             ('１６', 208),\n",
       "             ('中心', 209),\n",
       "             ('チーム', 210),\n",
       "             ('たち', 211),\n",
       "             ('北', 212),\n",
       "             ('分', 213),\n",
       "             ('られ', 214),\n",
       "             ('館', 215),\n",
       "             ('鉄道', 216),\n",
       "             ('おける', 217),\n",
       "             ('時間', 218),\n",
       "             ('以降', 219),\n",
       "             ('３０', 220),\n",
       "             ('ドイツ', 221),\n",
       "             ('小', 222),\n",
       "             ('出場', 223),\n",
       "             ('一部', 224),\n",
       "             ('南', 225),\n",
       "             ('用', 226),\n",
       "             ('さらに', 227),\n",
       "             ('！', 228),\n",
       "             ('発表', 229),\n",
       "             ('度', 230),\n",
       "             ('試合', 231),\n",
       "             ('平成', 232),\n",
       "             ('＝', 233),\n",
       "             ('だ', 234),\n",
       "             ('高', 235),\n",
       "             ('学', 236),\n",
       "             ('賞', 237),\n",
       "             ('局', 238),\n",
       "             ('登場', 239),\n",
       "             ('大会', 240),\n",
       "             ('版', 241),\n",
       "             ('開始', 242),\n",
       "             ('（）', 243),\n",
       "             ('次', 244),\n",
       "             ('フランス', 245),\n",
       "             ('川', 246),\n",
       "             ('際', 247),\n",
       "             ('点', 248),\n",
       "             ('式', 249),\n",
       "             ('関係', 250),\n",
       "             ('曲', 251),\n",
       "             ('参加', 252),\n",
       "             ('記録', 253),\n",
       "             ('体', 254),\n",
       "             ('ような', 255),\n",
       "             ('所属', 256),\n",
       "             ('％', 257),\n",
       "             ('多い', 258),\n",
       "             ('利用', 259),\n",
       "             ('ｍ', 260),\n",
       "             ('でき', 261),\n",
       "             ('だけ', 262),\n",
       "             ('世', 263),\n",
       "             ('．', 264),\n",
       "             ('１８', 265),\n",
       "             ('１７', 266),\n",
       "             ('シリーズ', 267),\n",
       "             ('明治', 268),\n",
       "             ('以上', 269),\n",
       "             ('事', 270),\n",
       "             ('ゲーム', 271),\n",
       "             ('見', 272),\n",
       "             ('お', 273),\n",
       "             ('力', 274),\n",
       "             ('##な', 275),\n",
       "             ('な', 276),\n",
       "             ('音楽', 277),\n",
       "             ('せ', 278),\n",
       "             ('シーズン', 279),\n",
       "             ('開催', 280),\n",
       "             ('##子', 281),\n",
       "             ('リーグ', 282),\n",
       "             ('島', 283),\n",
       "             ('ともに', 284),\n",
       "             ('にて', 285),\n",
       "             ('各', 286),\n",
       "             ('級', 287),\n",
       "             ('国際', 288),\n",
       "             ('いった', 289),\n",
       "             ('監督', 290),\n",
       "             ('氏', 291),\n",
       "             ('イギリス', 292),\n",
       "             ('山', 293),\n",
       "             ('両', 294),\n",
       "             ('世紀', 295),\n",
       "             ('問題', 296),\n",
       "             ('村', 297),\n",
       "             ('２０１０', 298),\n",
       "             ('旧', 299),\n",
       "             ('対して', 300),\n",
       "             ('優勝', 301),\n",
       "             ('知ら', 302),\n",
       "             ('都市', 303),\n",
       "             ('１９', 304),\n",
       "             ('行う', 305),\n",
       "             ('金', 306),\n",
       "             ('場', 307),\n",
       "             ('道', 308),\n",
       "             ('一般', 309),\n",
       "             ('中国', 310),\n",
       "             ('物', 311),\n",
       "             ('出演', 312),\n",
       "             ('設置', 313),\n",
       "             ('ので', 314),\n",
       "             ('せる', 315),\n",
       "             ('持つ', 316),\n",
       "             ('地方', 317),\n",
       "             ('事業', 318),\n",
       "             ('社会', 319),\n",
       "             ('卒業', 320),\n",
       "             ('戦争', 321),\n",
       "             ('共に', 322),\n",
       "             ('官', 323),\n",
       "             ('委員', 324),\n",
       "             ('２５', 325),\n",
       "             ('位置', 326),\n",
       "             ('計画', 327),\n",
       "             ('同じ', 328),\n",
       "             ('Ｂ', 329),\n",
       "             ('プロ', 330),\n",
       "             ('２００９', 331),\n",
       "             ('変更', 332),\n",
       "             ('Ａ', 333),\n",
       "             ('２００８', 334),\n",
       "             ('省', 335),\n",
       "             ('教育', 336),\n",
       "             ('結果', 337),\n",
       "             ('中央', 338),\n",
       "             ('２０１１', 339),\n",
       "             ('大阪', 340),\n",
       "             ('２００７', 341),\n",
       "             ('王', 342),\n",
       "             ('影響', 343),\n",
       "             ('アルバム', 344),\n",
       "             ('２１', 345),\n",
       "             ('期', 346),\n",
       "             ('文化', 347),\n",
       "             ('きた', 348),\n",
       "             ('頃', 349),\n",
       "             ('政府', 350),\n",
       "             ('隊', 351),\n",
       "             ('役', 352),\n",
       "             ('ほか', 353),\n",
       "             ('番', 354),\n",
       "             ('そして', 355),\n",
       "             ('２０１２', 356),\n",
       "             ('特に', 357),\n",
       "             ('獲得', 358),\n",
       "             ('ながら', 359),\n",
       "             ('Ｃ', 360),\n",
       "             ('##に', 361),\n",
       "             ('派', 362),\n",
       "             ('一方', 363),\n",
       "             ('手', 364),\n",
       "             ('建設', 365),\n",
       "             ('２４', 366),\n",
       "             ('いく', 367),\n",
       "             ('通り', 368),\n",
       "             ('##ラ', 369),\n",
       "             ('##ａ', 370),\n",
       "             ('なら', 371),\n",
       "             ('とき', 372),\n",
       "             ('自身', 373),\n",
       "             ('担当', 374),\n",
       "             ('外', 375),\n",
       "             ('考え', 376),\n",
       "             ('２００６', 377),\n",
       "             ('当初', 378),\n",
       "             ('ｋｍ', 379),\n",
       "             ('##ク', 380),\n",
       "             ('うち', 381),\n",
       "             ('設立', 382),\n",
       "             ('方', 383),\n",
       "             ('バス', 384),\n",
       "             ('##ｓ', 385),\n",
       "             ('機関', 386),\n",
       "             ('円', 387),\n",
       "             ('及び', 388),\n",
       "             ('種', 389),\n",
       "             ('事件', 390),\n",
       "             ('##ナ', 391),\n",
       "             ('経済', 392),\n",
       "             ('２２', 393),\n",
       "             ('２０１３', 394),\n",
       "             ('現', 395),\n",
       "             ('生', 396),\n",
       "             ('年間', 397),\n",
       "             ('２０１４', 398),\n",
       "             ('ア', 399),\n",
       "             ('かけて', 400),\n",
       "             ('０', 401),\n",
       "             ('情報', 402),\n",
       "             ('選挙', 403),\n",
       "             ('水', 404),\n",
       "             ('女性', 405),\n",
       "             ('都', 406),\n",
       "             ('政治', 407),\n",
       "             ('主義', 408),\n",
       "             ('販売', 409),\n",
       "             ('それぞれ', 410),\n",
       "             ('２３', 411),\n",
       "             ('意味', 412),\n",
       "             ('城', 413),\n",
       "             ('以外', 414),\n",
       "             ('地区', 415),\n",
       "             ('不', 416),\n",
       "             ('攻撃', 417),\n",
       "             ('２０１５', 418),\n",
       "             ('科', 419),\n",
       "             ('デビュー', 420),\n",
       "             ('##ｅ', 421),\n",
       "             ('##の', 422),\n",
       "             ('Ｓ', 423),\n",
       "             ('子', 424),\n",
       "             ('調査', 425),\n",
       "             ('艦', 426),\n",
       "             ('構成', 427),\n",
       "             ('グループ', 428),\n",
       "             ('##で', 429),\n",
       "             ('Ｄ', 430),\n",
       "             ('高等', 431),\n",
       "             ('米', 432),\n",
       "             ('かつて', 433),\n",
       "             ('２０１６', 434),\n",
       "             ('再', 435),\n",
       "             ('発生', 436),\n",
       "             ('##タ', 437),\n",
       "             ('２００５', 438),\n",
       "             ('アメリカ合衆国', 439),\n",
       "             ('##し', 440),\n",
       "             ('受け', 441),\n",
       "             ('技術', 442),\n",
       "             ('##ノ', 443),\n",
       "             ('##山', 444),\n",
       "             ('##り', 445),\n",
       "             ('可能', 446),\n",
       "             ('自分', 447),\n",
       "             ('##ｉ', 448),\n",
       "             ('または', 449),\n",
       "             ('航空', 450),\n",
       "             ('就任', 451),\n",
       "             ('父', 452),\n",
       "             ('##ｙ', 453),\n",
       "             ('##ｅｒ', 454),\n",
       "             ('２０００', 455),\n",
       "             ('路線', 456),\n",
       "             ('状態', 457),\n",
       "             ('##ｏ', 458),\n",
       "             ('歴史', 459),\n",
       "             ('話', 460),\n",
       "             ('システム', 461),\n",
       "             ('行って', 462),\n",
       "             ('場所', 463),\n",
       "             ('##ス', 464),\n",
       "             ('勝', 465),\n",
       "             ('採用', 466),\n",
       "             ('２６', 467),\n",
       "             ('行った', 468),\n",
       "             ('決定', 469),\n",
       "             ('##田', 470),\n",
       "             ('契約', 471),\n",
       "             ('２８', 472),\n",
       "             ('Ｍ', 473),\n",
       "             ('神', 474),\n",
       "             ('２０１７', 475),\n",
       "             ('最も', 476),\n",
       "             ('台', 477),\n",
       "             ('せた', 478),\n",
       "             ('制作', 479),\n",
       "             ('人口', 480),\n",
       "             ('２７', 481),\n",
       "             ('施設', 482),\n",
       "             ('正', 483),\n",
       "             ('ただし', 484),\n",
       "             ('公', 485),\n",
       "             ('終了', 486),\n",
       "             ('製造', 487),\n",
       "             ('対する', 488),\n",
       "             ('##リ', 489),\n",
       "             ('船', 490),\n",
       "             ('部分', 491),\n",
       "             ('関する', 492),\n",
       "             ('Ｐ', 493),\n",
       "             ('高校', 494),\n",
       "             ('##ト', 495),\n",
       "             ('総', 496),\n",
       "             ('石', 497),\n",
       "             ('組織', 498),\n",
       "             ('公開', 499),\n",
       "             ('ほど', 500),\n",
       "             ('教授', 501),\n",
       "             ('収録', 502),\n",
       "             ('Ｆ', 503),\n",
       "             ('無', 504),\n",
       "             ('株式', 505),\n",
       "             ('生産', 506),\n",
       "             ('ロシア', 507),\n",
       "             ('２９', 508),\n",
       "             ('車両', 509),\n",
       "             ('必要', 510),\n",
       "             ('最', 511),\n",
       "             ('使わ', 512),\n",
       "             ('海軍', 513),\n",
       "             ('全国', 514),\n",
       "             ('列車', 515),\n",
       "             ('帝国', 516),\n",
       "             ('##コ', 517),\n",
       "             ('野球', 518),\n",
       "             ('用い', 519),\n",
       "             ('書', 520),\n",
       "             ('##川', 521),\n",
       "             ('大きな', 522),\n",
       "             ('全て', 523),\n",
       "             ('Ｔ', 524),\n",
       "             ('ラ', 525),\n",
       "             ('初めて', 526),\n",
       "             ('シングル', 527),\n",
       "             ('モデル', 528),\n",
       "             ('枚', 529),\n",
       "             ('Ｋ', 530),\n",
       "             ('Ｒ', 531),\n",
       "             ('##フ', 532),\n",
       "             ('##ダ', 533),\n",
       "             ('面', 534),\n",
       "             ('四', 535),\n",
       "             ('Ｇ', 536),\n",
       "             ('高い', 537),\n",
       "             ('２００４', 538),\n",
       "             ('議員', 539),\n",
       "             ('部隊', 540),\n",
       "             ('率', 541),\n",
       "             ('主に', 542),\n",
       "             ('受賞', 543),\n",
       "             ('メンバー', 544),\n",
       "             ('名称', 545),\n",
       "             ('企業', 546),\n",
       "             ('〜', 547),\n",
       "             ('目的', 548),\n",
       "             ('最終', 549),\n",
       "             ('##ド', 550),\n",
       "             ('道路', 551),\n",
       "             ('##ティ', 552),\n",
       "             ('勝利', 553),\n",
       "             ('##ン', 554),\n",
       "             ('##カ', 555),\n",
       "             ('そこ', 556),\n",
       "             ('機能', 557),\n",
       "             ('センター', 558),\n",
       "             ('京都', 559),\n",
       "             ('運動', 560),\n",
       "             ('受けた', 561),\n",
       "             ('末', 562),\n",
       "             ('員', 563),\n",
       "             ('活躍', 564),\n",
       "             ('店', 565),\n",
       "             ('１００', 566),\n",
       "             ('管理', 567),\n",
       "             ('党', 568),\n",
       "             ('独立', 569),\n",
       "             ('実施', 570),\n",
       "             ('移籍', 571),\n",
       "             ('取り', 572),\n",
       "             ('海', 573),\n",
       "             ('##い', 574),\n",
       "             ('人物', 575),\n",
       "             ('時期', 576),\n",
       "             ('ほとんど', 577),\n",
       "             ('##ム', 578),\n",
       "             ('Ｊ', 579),\n",
       "             ('生活', 580),\n",
       "             ('務めた', 581),\n",
       "             ('戦い', 582),\n",
       "             ('Ｌ', 583),\n",
       "             ('経て', 584),\n",
       "             ('入り', 585),\n",
       "             ('通常', 586),\n",
       "             ('製作', 587),\n",
       "             ('Ｈ', 588),\n",
       "             ('##ル', 589),\n",
       "             ('サッカー', 590),\n",
       "             ('構造', 591),\n",
       "             ('馬', 592),\n",
       "             ('クラブ', 593),\n",
       "             ('年度', 594),\n",
       "             ('対応', 595),\n",
       "             ('教会', 596),\n",
       "             ('カ', 597),\n",
       "             ('##ロ', 598),\n",
       "             ('朝', 599),\n",
       "             ('成功', 600),\n",
       "             ('デ', 601),\n",
       "             ('エンジン', 602),\n",
       "             ('量', 603),\n",
       "             ('ところ', 604),\n",
       "             ('イタリア', 605),\n",
       "             ('発見', 606),\n",
       "             ('例', 607),\n",
       "             ('基本', 608),\n",
       "             ('含む', 609),\n",
       "             ('江戸', 610),\n",
       "             ('重', 611),\n",
       "             ('戦闘', 612),\n",
       "             ('受けて', 613),\n",
       "             ('##レ', 614),\n",
       "             ('府', 615),\n",
       "             ('以前', 616),\n",
       "             ('##って', 617),\n",
       "             ('##シ', 618),\n",
       "             ('Ｎ', 619),\n",
       "             ('##ズ', 620),\n",
       "             ('##リー', 621),\n",
       "             ('##ア', 622),\n",
       "             ('設定', 623),\n",
       "             ('３１', 624),\n",
       "             ('実際', 625),\n",
       "             ('キャラクター', 626),\n",
       "             ('搭載', 627),\n",
       "             ('名前', 628),\n",
       "             ('##ニ', 629),\n",
       "             ('非', 630),\n",
       "             ('ここ', 631),\n",
       "             ('規模', 632),\n",
       "             ('対', 633),\n",
       "             ('協会', 634),\n",
       "             ('##ら', 635),\n",
       "             ('２００３', 636),\n",
       "             ('アニメ', 637),\n",
       "             ('ホーム', 638),\n",
       "             ('内容', 639),\n",
       "             ('設計', 640),\n",
       "             ('アル', 641),\n",
       "             ('##った', 642),\n",
       "             ('##イ', 643),\n",
       "             ('##野', 644),\n",
       "             ('Ｖ', 645),\n",
       "             ('漫画', 646),\n",
       "             ('編成', 647),\n",
       "             ('決勝', 648),\n",
       "             ('##ツ', 649),\n",
       "             ('指定', 650),\n",
       "             ('リ', 651),\n",
       "             ('人間', 652),\n",
       "             ('室', 653),\n",
       "             ('作曲', 654),\n",
       "             ('エ', 655),\n",
       "             ('評価', 656),\n",
       "             ('##サ', 657),\n",
       "             ('特徴', 658),\n",
       "             ('ドラマ', 659),\n",
       "             ('##ジ', 660),\n",
       "             ('女子', 661),\n",
       "             ('指揮', 662),\n",
       "             ('科学', 663),\n",
       "             ('交通', 664),\n",
       "             ('マ', 665),\n",
       "             ('異なる', 666),\n",
       "             ('再び', 667),\n",
       "             ('##る', 668),\n",
       "             ('出版', 669),\n",
       "             ('最初の', 670),\n",
       "             ('運行', 671),\n",
       "             ('##き', 672),\n",
       "             ('専門', 673),\n",
       "             ('副', 674),\n",
       "             ('ヨーロッパ', 675),\n",
       "             ('##マ', 676),\n",
       "             ('##チ', 677),\n",
       "             ('先', 678),\n",
       "             ('指導', 679),\n",
       "             ('連合', 680),\n",
       "             ('運営', 681),\n",
       "             ('制', 682),\n",
       "             ('み', 683),\n",
       "             ('五', 684),\n",
       "             ('予定', 685),\n",
       "             ('国家', 686),\n",
       "             ('イ', 687),\n",
       "             ('光', 688),\n",
       "             ('期間', 689),\n",
       "             ('大きく', 690),\n",
       "             ('表記', 691),\n",
       "             ('主', 692),\n",
       "             ('対し', 693),\n",
       "             ('彼女', 694),\n",
       "             ('品', 695),\n",
       "             ('結婚', 696),\n",
       "             ('向け', 697),\n",
       "             ('翌', 698),\n",
       "             ('’', 699),\n",
       "             ('バンド', 700),\n",
       "             ('リリース', 701),\n",
       "             ('##Ｓ', 702),\n",
       "             ('対象', 703),\n",
       "             ('団', 704),\n",
       "             ('連続', 705),\n",
       "             ('５０', 706),\n",
       "             ('経営', 707),\n",
       "             ('歌', 708),\n",
       "             ('廃止', 709),\n",
       "             ('色', 710),\n",
       "             ('多数', 711),\n",
       "             ('##く', 712),\n",
       "             ('タイトル', 713),\n",
       "             ('団体', 714),\n",
       "             ('##ガ', 715),\n",
       "             ('##キ', 716),\n",
       "             ('時点', 717),\n",
       "             ('２００２', 718),\n",
       "             ('当', 719),\n",
       "             ('すべて', 720),\n",
       "             ('しまう', 721),\n",
       "             ('##ｔ', 722),\n",
       "             ('あるいは', 723),\n",
       "             ('条', 724),\n",
       "             ('##ブ', 725),\n",
       "             ('様々な', 726),\n",
       "             ('程度', 727),\n",
       "             ('小説', 728),\n",
       "             ('藩', 729),\n",
       "             ('記念', 730),\n",
       "             ('製', 731),\n",
       "             ('院', 732),\n",
       "             ('英語', 733),\n",
       "             ('参照', 734),\n",
       "             ('理由', 735),\n",
       "             ('大戦', 736),\n",
       "             ('持ち', 737),\n",
       "             ('##ｉｎ', 738),\n",
       "             ('事務', 739),\n",
       "             ('提供', 740),\n",
       "             ('ド', 741),\n",
       "             ('導入', 742),\n",
       "             ('自動車', 743),\n",
       "             ('群', 744),\n",
       "             ('Ｅ', 745),\n",
       "             ('１９９０', 746),\n",
       "             ('ローマ', 747),\n",
       "             ('２０１８', 748),\n",
       "             ('オ', 749),\n",
       "             ('##テ', 750),\n",
       "             ('死去', 751),\n",
       "             ('４０', 752),\n",
       "             ('人気', 753),\n",
       "             ('##ー', 754),\n",
       "             ('たい', 755),\n",
       "             ('２００１', 756),\n",
       "             ('達', 757),\n",
       "             ('##ディ', 758),\n",
       "             ('労働', 759),\n",
       "             ('##島', 760),\n",
       "             ('生まれ', 761),\n",
       "             ('よう', 762),\n",
       "             ('展開', 763),\n",
       "             ('##バ', 764),\n",
       "             ('周辺', 765),\n",
       "             ('ザ', 766),\n",
       "             ('全体', 767),\n",
       "             ('デザイン', 768),\n",
       "             ('運転', 769),\n",
       "             ('基', 770),\n",
       "             ('方法', 771),\n",
       "             ('領', 772),\n",
       "             ('サービス', 773),\n",
       "             ('今', 774),\n",
       "             ('##ｅｎ', 775),\n",
       "             ('その他', 776),\n",
       "             ('サン', 777),\n",
       "             ('形式', 778),\n",
       "             ('コ', 779),\n",
       "             ('真', 780),\n",
       "             ('公園', 781),\n",
       "             ('##ネ', 782),\n",
       "             ('橋', 783),\n",
       "             ('以来', 784),\n",
       "             ('##ｏｎ', 785),\n",
       "             ('白', 786),\n",
       "             ('ラジオ', 787),\n",
       "             ('小学校', 788),\n",
       "             ('サ', 789),\n",
       "             ('しか', 790),\n",
       "             ('説', 791),\n",
       "             ('兵', 792),\n",
       "             ('結成', 793),\n",
       "             ('##一', 794),\n",
       "             ('議会', 795),\n",
       "             ('総合', 796),\n",
       "             ('同時に', 797),\n",
       "             ('大統領', 798),\n",
       "             ('##ａｌ', 799),\n",
       "             ('のち', 800),\n",
       "             ('##人', 801),\n",
       "             ('マン', 802),\n",
       "             ('##Ｌ', 803),\n",
       "             ('警察', 804),\n",
       "             ('舞台', 805),\n",
       "             ('環境', 806),\n",
       "             ('言わ', 807),\n",
       "             ('宇宙', 808),\n",
       "             ('支援', 809),\n",
       "             ('引退', 810),\n",
       "             ('##ｄ', 811),\n",
       "             ('娘', 812),\n",
       "             ('##グ', 813),\n",
       "             ('関連', 814),\n",
       "             ('中学校', 815),\n",
       "             ('最高', 816),\n",
       "             ('国民', 817),\n",
       "             ('由来', 818),\n",
       "             ('会議', 819),\n",
       "             ('初期', 820),\n",
       "             ('スーパー', 821),\n",
       "             ('持って', 822),\n",
       "             ('ほぼ', 823),\n",
       "             ('競技', 824),\n",
       "             ('自治', 825),\n",
       "             ('飛行', 826),\n",
       "             ('自ら', 827),\n",
       "             ('連邦', 828),\n",
       "             ('バ', 829),\n",
       "             ('スポーツ', 830),\n",
       "             ('##ｎ', 831),\n",
       "             ('完成', 832),\n",
       "             ('いずれ', 833),\n",
       "             ('含ま', 834),\n",
       "             ('夏', 835),\n",
       "             ('北海道', 836),\n",
       "             ('裁判', 837),\n",
       "             ('ドル', 838),\n",
       "             ('陸軍', 839),\n",
       "             ('##Ｔ', 840),\n",
       "             ('組', 841),\n",
       "             ('街', 842),\n",
       "             ('##Ｃ', 843),\n",
       "             ('ひと', 844),\n",
       "             ('行い', 845),\n",
       "             ('＿', 846),\n",
       "             ('カード', 847),\n",
       "             ('##ウ', 848),\n",
       "             ('ｃｍ', 849),\n",
       "             ('ル', 850),\n",
       "             ('変化', 851),\n",
       "             ('１９８０', 852),\n",
       "             ('巻', 853),\n",
       "             ('共同', 854),\n",
       "             ('特別', 855),\n",
       "             ('##か', 856),\n",
       "             ('寺', 857),\n",
       "             ('支配', 858),\n",
       "             ('花', 859),\n",
       "             ('士', 860),\n",
       "             ('スペイン', 861),\n",
       "             ('主張', 862),\n",
       "             ('大正', 863),\n",
       "             ('オリンピック', 864),\n",
       "             ('類', 865),\n",
       "             ('天', 866),\n",
       "             ('学者', 867),\n",
       "             ('##さ', 868),\n",
       "             ('アン', 869),\n",
       "             ('Ｗ', 870),\n",
       "             ('##み', 871),\n",
       "             ('対戦', 872),\n",
       "             ('とって', 873),\n",
       "             ('##ａｎ', 874),\n",
       "             ('妻', 875),\n",
       "             ('丁目', 876),\n",
       "             ('アジア', 877),\n",
       "             ('パ', 878),\n",
       "             ('文', 879),\n",
       "             ('校', 880),\n",
       "             ('個', 881),\n",
       "             ('##ｅｓ', 882),\n",
       "             ('建築', 883),\n",
       "             ('##Ｂ', 884),\n",
       "             ('頭', 885),\n",
       "             ('よく', 886),\n",
       "             ('整備', 887),\n",
       "             ('撮影', 888),\n",
       "             ('##ミ', 889),\n",
       "             ('形成', 890),\n",
       "             ('作戦', 891),\n",
       "             ('演奏', 892),\n",
       "             ('共和', 893),\n",
       "             ('##プ', 894),\n",
       "             ('レ', 895),\n",
       "             ('英', 896),\n",
       "             ('神社', 897),\n",
       "             ('移動', 898),\n",
       "             ('右', 899),\n",
       "             ('輸送', 900),\n",
       "             ('与え', 901),\n",
       "             ('##Ｏ', 902),\n",
       "             ('##Ｅ', 903),\n",
       "             ('制度', 904),\n",
       "             ('１９９９', 905),\n",
       "             ('ニューヨーク', 906),\n",
       "             ('法人', 907),\n",
       "             ('能力', 908),\n",
       "             ('発展', 909),\n",
       "             ('左', 910),\n",
       "             ('営業', 911),\n",
       "             ('レース', 912),\n",
       "             ('非常に', 913),\n",
       "             ('ハ', 914),\n",
       "             ('文字', 915),\n",
       "             ('登録', 916),\n",
       "             ('##マン', 917),\n",
       "             ('子供', 918),\n",
       "             ('状況', 919),\n",
       "             ('##木', 920),\n",
       "             ('伴い', 921),\n",
       "             ('準', 922),\n",
       "             ('ス', 923),\n",
       "             ('風', 924),\n",
       "             ('表現', 925),\n",
       "             ('母', 926),\n",
       "             ('##Ｄ', 927),\n",
       "             ('ファン', 928),\n",
       "             ('産業', 929),\n",
       "             ('韓国', 930),\n",
       "             ('まま', 931),\n",
       "             ('表示', 932),\n",
       "             ('イン', 933),\n",
       "             ('圏', 934),\n",
       "             ('企画', 935),\n",
       "             ('以後', 936),\n",
       "             ('試験', 937),\n",
       "             ('##ゴ', 938),\n",
       "             ('公式', 939),\n",
       "             ('着', 940),\n",
       "             ('得', 941),\n",
       "             ('##Ｆ', 942),\n",
       "             ('男', 943),\n",
       "             ('データ', 944),\n",
       "             ('発行', 945),\n",
       "             ('行政', 946),\n",
       "             ('##トン', 947),\n",
       "             ('誌', 948),\n",
       "             ('区間', 949),\n",
       "             ('##す', 950),\n",
       "             ('##ラー', 951),\n",
       "             ('##デ', 952),\n",
       "             ('##Ｇ', 953),\n",
       "             ('す', 954),\n",
       "             ('御', 955),\n",
       "             ('春', 956),\n",
       "             ('論', 957),\n",
       "             ('姿', 958),\n",
       "             ('合併', 959),\n",
       "             ('##Ｋ', 960),\n",
       "             ('距離', 961),\n",
       "             ('複数', 962),\n",
       "             ('心', 963),\n",
       "             ('##である', 964),\n",
       "             ('##ｕ', 965),\n",
       "             ('認め', 966),\n",
       "             ('新しい', 967),\n",
       "             ('方面', 968),\n",
       "             ('##ｍ', 969),\n",
       "             ('##スト', 970),\n",
       "             ('選出', 971),\n",
       "             ('分類', 972),\n",
       "             ('種類', 973),\n",
       "             ('広島', 974),\n",
       "             ('口', 975),\n",
       "             ('１９７０', 976),\n",
       "             ('ｍｍ', 977),\n",
       "             ('##オ', 978),\n",
       "             ('階', 979),\n",
       "             ('新聞', 980),\n",
       "             ('##リン', 981),\n",
       "             ('赤', 982),\n",
       "             ('シ', 983),\n",
       "             ('番号', 984),\n",
       "             ('##ｈ', 985),\n",
       "             ('成立', 986),\n",
       "             ('工業', 987),\n",
       "             ('##ｉｎｇ', 988),\n",
       "             ('##れ', 989),\n",
       "             ('なければ', 990),\n",
       "             ('工場', 991),\n",
       "             ('運用', 992),\n",
       "             ('言語', 993),\n",
       "             ('自由', 994),\n",
       "             ('##ｌ', 995),\n",
       "             ('##ｃ', 996),\n",
       "             ('南部', 997),\n",
       "             ('装置', 998),\n",
       "             ('##ｋ', 999),\n",
       "             ...])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, '[PAD]'),\n",
       "             (1, '[UNK]'),\n",
       "             (2, '[CLS]'),\n",
       "             (3, '[SEP]'),\n",
       "             (4, '[MASK]'),\n",
       "             (5, 'の'),\n",
       "             (6, '、'),\n",
       "             (7, '。'),\n",
       "             (8, 'に'),\n",
       "             (9, 'は'),\n",
       "             (10, 'を'),\n",
       "             (11, 'が'),\n",
       "             (12, 'と'),\n",
       "             (13, 'で'),\n",
       "             (14, '年'),\n",
       "             (15, '・'),\n",
       "             (16, '（'),\n",
       "             (17, '）'),\n",
       "             (18, 'さ'),\n",
       "             (19, 'して'),\n",
       "             (20, 'した'),\n",
       "             (21, 'いる'),\n",
       "             (22, 'する'),\n",
       "             (23, 'も'),\n",
       "             (24, '「'),\n",
       "             (25, '」'),\n",
       "             (26, '月'),\n",
       "             (27, 'から'),\n",
       "             (28, 'れた'),\n",
       "             (29, '日'),\n",
       "             (30, 'こと'),\n",
       "             (31, 'し'),\n",
       "             (32, 'である'),\n",
       "             (33, 'れて'),\n",
       "             (34, 'や'),\n",
       "             (35, '２'),\n",
       "             (36, '１'),\n",
       "             (37, 'いた'),\n",
       "             (38, 'ある'),\n",
       "             (39, '『'),\n",
       "             (40, '』'),\n",
       "             (41, 'れる'),\n",
       "             (42, 'など'),\n",
       "             (43, '３'),\n",
       "             (44, '−'),\n",
       "             (45, ''),\n",
       "             (46, 'この'),\n",
       "             (47, 'ない'),\n",
       "             (48, 'ため'),\n",
       "             (49, '日本'),\n",
       "             (50, '人'),\n",
       "             (51, '”'),\n",
       "             (52, 'より'),\n",
       "             (53, '４'),\n",
       "             (54, 'れ'),\n",
       "             (55, '第'),\n",
       "             (56, 'いう'),\n",
       "             (57, '者'),\n",
       "             (58, 'その'),\n",
       "             (59, 'なった'),\n",
       "             (60, 'もの'),\n",
       "             (61, 'へ'),\n",
       "             (62, '後'),\n",
       "             (63, 'まで'),\n",
       "             (64, 'また'),\n",
       "             (65, '市'),\n",
       "             (66, 'なる'),\n",
       "             (67, '５'),\n",
       "             (68, '中'),\n",
       "             (69, '６'),\n",
       "             (70, '一'),\n",
       "             (71, '同'),\n",
       "             (72, '県'),\n",
       "             (73, 'これ'),\n",
       "             (74, '１０'),\n",
       "             (75, '７'),\n",
       "             (76, '内'),\n",
       "             (77, '８'),\n",
       "             (78, 'なって'),\n",
       "             (79, 'おり'),\n",
       "             (80, 'よる'),\n",
       "             (81, '９'),\n",
       "             (82, '大学'),\n",
       "             (83, 'つ'),\n",
       "             (84, '大'),\n",
       "             (85, '国'),\n",
       "             (86, 'よって'),\n",
       "             (87, '時'),\n",
       "             (88, '１２'),\n",
       "             (89, 'であった'),\n",
       "             (90, 'か'),\n",
       "             (91, '家'),\n",
       "             (92, '駅'),\n",
       "             (93, 'ように'),\n",
       "             (94, 'ら'),\n",
       "             (95, '現在'),\n",
       "             (96, '的な'),\n",
       "             (97, '本'),\n",
       "             (98, '１１'),\n",
       "             (99, '軍'),\n",
       "             (100, '上'),\n",
       "             (101, '：'),\n",
       "             (102, '化'),\n",
       "             (103, 'であり'),\n",
       "             (104, '的に'),\n",
       "             (105, 'なり'),\n",
       "             (106, '放送'),\n",
       "             (107, '名'),\n",
       "             (108, '性'),\n",
       "             (109, 'ず'),\n",
       "             (110, '部'),\n",
       "             (111, '回'),\n",
       "             (112, '目'),\n",
       "             (113, '町'),\n",
       "             (114, '時代'),\n",
       "             (115, 'それ'),\n",
       "             (116, 'なかった'),\n",
       "             (117, 'おいて'),\n",
       "             (118, '世界'),\n",
       "             (119, '代'),\n",
       "             (120, '線'),\n",
       "             (121, '間'),\n",
       "             (122, '戦'),\n",
       "             (123, 'でも'),\n",
       "             (124, 'られる'),\n",
       "             (125, 'あり'),\n",
       "             (126, '会'),\n",
       "             (127, '場合'),\n",
       "             (128, '行わ'),\n",
       "             (129, '二'),\n",
       "             (130, 'ついて'),\n",
       "             (131, '所'),\n",
       "             (132, 'その後'),\n",
       "             (133, '東京'),\n",
       "             (134, '前'),\n",
       "             (135, '多く'),\n",
       "             (136, '州'),\n",
       "             (137, 'だった'),\n",
       "             (138, '地'),\n",
       "             (139, 'あった'),\n",
       "             (140, 'なく'),\n",
       "             (141, 'しかし'),\n",
       "             (142, 'い'),\n",
       "             (143, 'られた'),\n",
       "             (144, '号'),\n",
       "             (145, '数'),\n",
       "             (146, 'できる'),\n",
       "             (147, '的'),\n",
       "             (148, '作品'),\n",
       "             (149, '彼'),\n",
       "             (150, '選手'),\n",
       "             (151, '他'),\n",
       "             (152, '｜'),\n",
       "             (153, '使用'),\n",
       "             (154, '機'),\n",
       "             (155, '昭和'),\n",
       "             (156, '語'),\n",
       "             (157, 'られて'),\n",
       "             (158, '郡'),\n",
       "             (159, '位'),\n",
       "             (160, '研究'),\n",
       "             (161, '当時'),\n",
       "             (162, '存在'),\n",
       "             (163, '新'),\n",
       "             (164, '元'),\n",
       "             (165, 'アメリカ'),\n",
       "             (166, '長'),\n",
       "             (167, '側'),\n",
       "             (168, '三'),\n",
       "             (169, '活動'),\n",
       "             (170, '映画'),\n",
       "             (171, '初'),\n",
       "             (172, '学校'),\n",
       "             (173, '社'),\n",
       "             (174, '等'),\n",
       "             (175, '１５'),\n",
       "             (176, '，'),\n",
       "             (177, '全'),\n",
       "             (178, '下'),\n",
       "             (179, '番組'),\n",
       "             (180, '呼ば'),\n",
       "             (181, '東'),\n",
       "             (182, '区'),\n",
       "             (183, '２０'),\n",
       "             (184, '会社'),\n",
       "             (185, '出身'),\n",
       "             (186, 'および'),\n",
       "             (187, '／'),\n",
       "             (188, '車'),\n",
       "             (189, '約'),\n",
       "             (190, 'のみ'),\n",
       "             (191, '代表'),\n",
       "             (192, '形'),\n",
       "             (193, '権'),\n",
       "             (194, 'なお'),\n",
       "             (195, 'テレビ'),\n",
       "             (196, '西'),\n",
       "             (197, '系'),\n",
       "             (198, '発売'),\n",
       "             (199, '型'),\n",
       "             (200, '以下'),\n",
       "             (201, '地域'),\n",
       "             (202, '法'),\n",
       "             (203, '開発'),\n",
       "             (204, '１４'),\n",
       "             (205, '歳'),\n",
       "             (206, '作'),\n",
       "             (207, '１３'),\n",
       "             (208, '１６'),\n",
       "             (209, '中心'),\n",
       "             (210, 'チーム'),\n",
       "             (211, 'たち'),\n",
       "             (212, '北'),\n",
       "             (213, '分'),\n",
       "             (214, 'られ'),\n",
       "             (215, '館'),\n",
       "             (216, '鉄道'),\n",
       "             (217, 'おける'),\n",
       "             (218, '時間'),\n",
       "             (219, '以降'),\n",
       "             (220, '３０'),\n",
       "             (221, 'ドイツ'),\n",
       "             (222, '小'),\n",
       "             (223, '出場'),\n",
       "             (224, '一部'),\n",
       "             (225, '南'),\n",
       "             (226, '用'),\n",
       "             (227, 'さらに'),\n",
       "             (228, '！'),\n",
       "             (229, '発表'),\n",
       "             (230, '度'),\n",
       "             (231, '試合'),\n",
       "             (232, '平成'),\n",
       "             (233, '＝'),\n",
       "             (234, 'だ'),\n",
       "             (235, '高'),\n",
       "             (236, '学'),\n",
       "             (237, '賞'),\n",
       "             (238, '局'),\n",
       "             (239, '登場'),\n",
       "             (240, '大会'),\n",
       "             (241, '版'),\n",
       "             (242, '開始'),\n",
       "             (243, '（）'),\n",
       "             (244, '次'),\n",
       "             (245, 'フランス'),\n",
       "             (246, '川'),\n",
       "             (247, '際'),\n",
       "             (248, '点'),\n",
       "             (249, '式'),\n",
       "             (250, '関係'),\n",
       "             (251, '曲'),\n",
       "             (252, '参加'),\n",
       "             (253, '記録'),\n",
       "             (254, '体'),\n",
       "             (255, 'ような'),\n",
       "             (256, '所属'),\n",
       "             (257, '％'),\n",
       "             (258, '多い'),\n",
       "             (259, '利用'),\n",
       "             (260, 'ｍ'),\n",
       "             (261, 'でき'),\n",
       "             (262, 'だけ'),\n",
       "             (263, '世'),\n",
       "             (264, '．'),\n",
       "             (265, '１８'),\n",
       "             (266, '１７'),\n",
       "             (267, 'シリーズ'),\n",
       "             (268, '明治'),\n",
       "             (269, '以上'),\n",
       "             (270, '事'),\n",
       "             (271, 'ゲーム'),\n",
       "             (272, '見'),\n",
       "             (273, 'お'),\n",
       "             (274, '力'),\n",
       "             (275, '##な'),\n",
       "             (276, 'な'),\n",
       "             (277, '音楽'),\n",
       "             (278, 'せ'),\n",
       "             (279, 'シーズン'),\n",
       "             (280, '開催'),\n",
       "             (281, '##子'),\n",
       "             (282, 'リーグ'),\n",
       "             (283, '島'),\n",
       "             (284, 'ともに'),\n",
       "             (285, 'にて'),\n",
       "             (286, '各'),\n",
       "             (287, '級'),\n",
       "             (288, '国際'),\n",
       "             (289, 'いった'),\n",
       "             (290, '監督'),\n",
       "             (291, '氏'),\n",
       "             (292, 'イギリス'),\n",
       "             (293, '山'),\n",
       "             (294, '両'),\n",
       "             (295, '世紀'),\n",
       "             (296, '問題'),\n",
       "             (297, '村'),\n",
       "             (298, '２０１０'),\n",
       "             (299, '旧'),\n",
       "             (300, '対して'),\n",
       "             (301, '優勝'),\n",
       "             (302, '知ら'),\n",
       "             (303, '都市'),\n",
       "             (304, '１９'),\n",
       "             (305, '行う'),\n",
       "             (306, '金'),\n",
       "             (307, '場'),\n",
       "             (308, '道'),\n",
       "             (309, '一般'),\n",
       "             (310, '中国'),\n",
       "             (311, '物'),\n",
       "             (312, '出演'),\n",
       "             (313, '設置'),\n",
       "             (314, 'ので'),\n",
       "             (315, 'せる'),\n",
       "             (316, '持つ'),\n",
       "             (317, '地方'),\n",
       "             (318, '事業'),\n",
       "             (319, '社会'),\n",
       "             (320, '卒業'),\n",
       "             (321, '戦争'),\n",
       "             (322, '共に'),\n",
       "             (323, '官'),\n",
       "             (324, '委員'),\n",
       "             (325, '２５'),\n",
       "             (326, '位置'),\n",
       "             (327, '計画'),\n",
       "             (328, '同じ'),\n",
       "             (329, 'Ｂ'),\n",
       "             (330, 'プロ'),\n",
       "             (331, '２００９'),\n",
       "             (332, '変更'),\n",
       "             (333, 'Ａ'),\n",
       "             (334, '２００８'),\n",
       "             (335, '省'),\n",
       "             (336, '教育'),\n",
       "             (337, '結果'),\n",
       "             (338, '中央'),\n",
       "             (339, '２０１１'),\n",
       "             (340, '大阪'),\n",
       "             (341, '２００７'),\n",
       "             (342, '王'),\n",
       "             (343, '影響'),\n",
       "             (344, 'アルバム'),\n",
       "             (345, '２１'),\n",
       "             (346, '期'),\n",
       "             (347, '文化'),\n",
       "             (348, 'きた'),\n",
       "             (349, '頃'),\n",
       "             (350, '政府'),\n",
       "             (351, '隊'),\n",
       "             (352, '役'),\n",
       "             (353, 'ほか'),\n",
       "             (354, '番'),\n",
       "             (355, 'そして'),\n",
       "             (356, '２０１２'),\n",
       "             (357, '特に'),\n",
       "             (358, '獲得'),\n",
       "             (359, 'ながら'),\n",
       "             (360, 'Ｃ'),\n",
       "             (361, '##に'),\n",
       "             (362, '派'),\n",
       "             (363, '一方'),\n",
       "             (364, '手'),\n",
       "             (365, '建設'),\n",
       "             (366, '２４'),\n",
       "             (367, 'いく'),\n",
       "             (368, '通り'),\n",
       "             (369, '##ラ'),\n",
       "             (370, '##ａ'),\n",
       "             (371, 'なら'),\n",
       "             (372, 'とき'),\n",
       "             (373, '自身'),\n",
       "             (374, '担当'),\n",
       "             (375, '外'),\n",
       "             (376, '考え'),\n",
       "             (377, '２００６'),\n",
       "             (378, '当初'),\n",
       "             (379, 'ｋｍ'),\n",
       "             (380, '##ク'),\n",
       "             (381, 'うち'),\n",
       "             (382, '設立'),\n",
       "             (383, '方'),\n",
       "             (384, 'バス'),\n",
       "             (385, '##ｓ'),\n",
       "             (386, '機関'),\n",
       "             (387, '円'),\n",
       "             (388, '及び'),\n",
       "             (389, '種'),\n",
       "             (390, '事件'),\n",
       "             (391, '##ナ'),\n",
       "             (392, '経済'),\n",
       "             (393, '２２'),\n",
       "             (394, '２０１３'),\n",
       "             (395, '現'),\n",
       "             (396, '生'),\n",
       "             (397, '年間'),\n",
       "             (398, '２０１４'),\n",
       "             (399, 'ア'),\n",
       "             (400, 'かけて'),\n",
       "             (401, '０'),\n",
       "             (402, '情報'),\n",
       "             (403, '選挙'),\n",
       "             (404, '水'),\n",
       "             (405, '女性'),\n",
       "             (406, '都'),\n",
       "             (407, '政治'),\n",
       "             (408, '主義'),\n",
       "             (409, '販売'),\n",
       "             (410, 'それぞれ'),\n",
       "             (411, '２３'),\n",
       "             (412, '意味'),\n",
       "             (413, '城'),\n",
       "             (414, '以外'),\n",
       "             (415, '地区'),\n",
       "             (416, '不'),\n",
       "             (417, '攻撃'),\n",
       "             (418, '２０１５'),\n",
       "             (419, '科'),\n",
       "             (420, 'デビュー'),\n",
       "             (421, '##ｅ'),\n",
       "             (422, '##の'),\n",
       "             (423, 'Ｓ'),\n",
       "             (424, '子'),\n",
       "             (425, '調査'),\n",
       "             (426, '艦'),\n",
       "             (427, '構成'),\n",
       "             (428, 'グループ'),\n",
       "             (429, '##で'),\n",
       "             (430, 'Ｄ'),\n",
       "             (431, '高等'),\n",
       "             (432, '米'),\n",
       "             (433, 'かつて'),\n",
       "             (434, '２０１６'),\n",
       "             (435, '再'),\n",
       "             (436, '発生'),\n",
       "             (437, '##タ'),\n",
       "             (438, '２００５'),\n",
       "             (439, 'アメリカ合衆国'),\n",
       "             (440, '##し'),\n",
       "             (441, '受け'),\n",
       "             (442, '技術'),\n",
       "             (443, '##ノ'),\n",
       "             (444, '##山'),\n",
       "             (445, '##り'),\n",
       "             (446, '可能'),\n",
       "             (447, '自分'),\n",
       "             (448, '##ｉ'),\n",
       "             (449, 'または'),\n",
       "             (450, '航空'),\n",
       "             (451, '就任'),\n",
       "             (452, '父'),\n",
       "             (453, '##ｙ'),\n",
       "             (454, '##ｅｒ'),\n",
       "             (455, '２０００'),\n",
       "             (456, '路線'),\n",
       "             (457, '状態'),\n",
       "             (458, '##ｏ'),\n",
       "             (459, '歴史'),\n",
       "             (460, '話'),\n",
       "             (461, 'システム'),\n",
       "             (462, '行って'),\n",
       "             (463, '場所'),\n",
       "             (464, '##ス'),\n",
       "             (465, '勝'),\n",
       "             (466, '採用'),\n",
       "             (467, '２６'),\n",
       "             (468, '行った'),\n",
       "             (469, '決定'),\n",
       "             (470, '##田'),\n",
       "             (471, '契約'),\n",
       "             (472, '２８'),\n",
       "             (473, 'Ｍ'),\n",
       "             (474, '神'),\n",
       "             (475, '２０１７'),\n",
       "             (476, '最も'),\n",
       "             (477, '台'),\n",
       "             (478, 'せた'),\n",
       "             (479, '制作'),\n",
       "             (480, '人口'),\n",
       "             (481, '２７'),\n",
       "             (482, '施設'),\n",
       "             (483, '正'),\n",
       "             (484, 'ただし'),\n",
       "             (485, '公'),\n",
       "             (486, '終了'),\n",
       "             (487, '製造'),\n",
       "             (488, '対する'),\n",
       "             (489, '##リ'),\n",
       "             (490, '船'),\n",
       "             (491, '部分'),\n",
       "             (492, '関する'),\n",
       "             (493, 'Ｐ'),\n",
       "             (494, '高校'),\n",
       "             (495, '##ト'),\n",
       "             (496, '総'),\n",
       "             (497, '石'),\n",
       "             (498, '組織'),\n",
       "             (499, '公開'),\n",
       "             (500, 'ほど'),\n",
       "             (501, '教授'),\n",
       "             (502, '収録'),\n",
       "             (503, 'Ｆ'),\n",
       "             (504, '無'),\n",
       "             (505, '株式'),\n",
       "             (506, '生産'),\n",
       "             (507, 'ロシア'),\n",
       "             (508, '２９'),\n",
       "             (509, '車両'),\n",
       "             (510, '必要'),\n",
       "             (511, '最'),\n",
       "             (512, '使わ'),\n",
       "             (513, '海軍'),\n",
       "             (514, '全国'),\n",
       "             (515, '列車'),\n",
       "             (516, '帝国'),\n",
       "             (517, '##コ'),\n",
       "             (518, '野球'),\n",
       "             (519, '用い'),\n",
       "             (520, '書'),\n",
       "             (521, '##川'),\n",
       "             (522, '大きな'),\n",
       "             (523, '全て'),\n",
       "             (524, 'Ｔ'),\n",
       "             (525, 'ラ'),\n",
       "             (526, '初めて'),\n",
       "             (527, 'シングル'),\n",
       "             (528, 'モデル'),\n",
       "             (529, '枚'),\n",
       "             (530, 'Ｋ'),\n",
       "             (531, 'Ｒ'),\n",
       "             (532, '##フ'),\n",
       "             (533, '##ダ'),\n",
       "             (534, '面'),\n",
       "             (535, '四'),\n",
       "             (536, 'Ｇ'),\n",
       "             (537, '高い'),\n",
       "             (538, '２００４'),\n",
       "             (539, '議員'),\n",
       "             (540, '部隊'),\n",
       "             (541, '率'),\n",
       "             (542, '主に'),\n",
       "             (543, '受賞'),\n",
       "             (544, 'メンバー'),\n",
       "             (545, '名称'),\n",
       "             (546, '企業'),\n",
       "             (547, '〜'),\n",
       "             (548, '目的'),\n",
       "             (549, '最終'),\n",
       "             (550, '##ド'),\n",
       "             (551, '道路'),\n",
       "             (552, '##ティ'),\n",
       "             (553, '勝利'),\n",
       "             (554, '##ン'),\n",
       "             (555, '##カ'),\n",
       "             (556, 'そこ'),\n",
       "             (557, '機能'),\n",
       "             (558, 'センター'),\n",
       "             (559, '京都'),\n",
       "             (560, '運動'),\n",
       "             (561, '受けた'),\n",
       "             (562, '末'),\n",
       "             (563, '員'),\n",
       "             (564, '活躍'),\n",
       "             (565, '店'),\n",
       "             (566, '１００'),\n",
       "             (567, '管理'),\n",
       "             (568, '党'),\n",
       "             (569, '独立'),\n",
       "             (570, '実施'),\n",
       "             (571, '移籍'),\n",
       "             (572, '取り'),\n",
       "             (573, '海'),\n",
       "             (574, '##い'),\n",
       "             (575, '人物'),\n",
       "             (576, '時期'),\n",
       "             (577, 'ほとんど'),\n",
       "             (578, '##ム'),\n",
       "             (579, 'Ｊ'),\n",
       "             (580, '生活'),\n",
       "             (581, '務めた'),\n",
       "             (582, '戦い'),\n",
       "             (583, 'Ｌ'),\n",
       "             (584, '経て'),\n",
       "             (585, '入り'),\n",
       "             (586, '通常'),\n",
       "             (587, '製作'),\n",
       "             (588, 'Ｈ'),\n",
       "             (589, '##ル'),\n",
       "             (590, 'サッカー'),\n",
       "             (591, '構造'),\n",
       "             (592, '馬'),\n",
       "             (593, 'クラブ'),\n",
       "             (594, '年度'),\n",
       "             (595, '対応'),\n",
       "             (596, '教会'),\n",
       "             (597, 'カ'),\n",
       "             (598, '##ロ'),\n",
       "             (599, '朝'),\n",
       "             (600, '成功'),\n",
       "             (601, 'デ'),\n",
       "             (602, 'エンジン'),\n",
       "             (603, '量'),\n",
       "             (604, 'ところ'),\n",
       "             (605, 'イタリア'),\n",
       "             (606, '発見'),\n",
       "             (607, '例'),\n",
       "             (608, '基本'),\n",
       "             (609, '含む'),\n",
       "             (610, '江戸'),\n",
       "             (611, '重'),\n",
       "             (612, '戦闘'),\n",
       "             (613, '受けて'),\n",
       "             (614, '##レ'),\n",
       "             (615, '府'),\n",
       "             (616, '以前'),\n",
       "             (617, '##って'),\n",
       "             (618, '##シ'),\n",
       "             (619, 'Ｎ'),\n",
       "             (620, '##ズ'),\n",
       "             (621, '##リー'),\n",
       "             (622, '##ア'),\n",
       "             (623, '設定'),\n",
       "             (624, '３１'),\n",
       "             (625, '実際'),\n",
       "             (626, 'キャラクター'),\n",
       "             (627, '搭載'),\n",
       "             (628, '名前'),\n",
       "             (629, '##ニ'),\n",
       "             (630, '非'),\n",
       "             (631, 'ここ'),\n",
       "             (632, '規模'),\n",
       "             (633, '対'),\n",
       "             (634, '協会'),\n",
       "             (635, '##ら'),\n",
       "             (636, '２００３'),\n",
       "             (637, 'アニメ'),\n",
       "             (638, 'ホーム'),\n",
       "             (639, '内容'),\n",
       "             (640, '設計'),\n",
       "             (641, 'アル'),\n",
       "             (642, '##った'),\n",
       "             (643, '##イ'),\n",
       "             (644, '##野'),\n",
       "             (645, 'Ｖ'),\n",
       "             (646, '漫画'),\n",
       "             (647, '編成'),\n",
       "             (648, '決勝'),\n",
       "             (649, '##ツ'),\n",
       "             (650, '指定'),\n",
       "             (651, 'リ'),\n",
       "             (652, '人間'),\n",
       "             (653, '室'),\n",
       "             (654, '作曲'),\n",
       "             (655, 'エ'),\n",
       "             (656, '評価'),\n",
       "             (657, '##サ'),\n",
       "             (658, '特徴'),\n",
       "             (659, 'ドラマ'),\n",
       "             (660, '##ジ'),\n",
       "             (661, '女子'),\n",
       "             (662, '指揮'),\n",
       "             (663, '科学'),\n",
       "             (664, '交通'),\n",
       "             (665, 'マ'),\n",
       "             (666, '異なる'),\n",
       "             (667, '再び'),\n",
       "             (668, '##る'),\n",
       "             (669, '出版'),\n",
       "             (670, '最初の'),\n",
       "             (671, '運行'),\n",
       "             (672, '##き'),\n",
       "             (673, '専門'),\n",
       "             (674, '副'),\n",
       "             (675, 'ヨーロッパ'),\n",
       "             (676, '##マ'),\n",
       "             (677, '##チ'),\n",
       "             (678, '先'),\n",
       "             (679, '指導'),\n",
       "             (680, '連合'),\n",
       "             (681, '運営'),\n",
       "             (682, '制'),\n",
       "             (683, 'み'),\n",
       "             (684, '五'),\n",
       "             (685, '予定'),\n",
       "             (686, '国家'),\n",
       "             (687, 'イ'),\n",
       "             (688, '光'),\n",
       "             (689, '期間'),\n",
       "             (690, '大きく'),\n",
       "             (691, '表記'),\n",
       "             (692, '主'),\n",
       "             (693, '対し'),\n",
       "             (694, '彼女'),\n",
       "             (695, '品'),\n",
       "             (696, '結婚'),\n",
       "             (697, '向け'),\n",
       "             (698, '翌'),\n",
       "             (699, '’'),\n",
       "             (700, 'バンド'),\n",
       "             (701, 'リリース'),\n",
       "             (702, '##Ｓ'),\n",
       "             (703, '対象'),\n",
       "             (704, '団'),\n",
       "             (705, '連続'),\n",
       "             (706, '５０'),\n",
       "             (707, '経営'),\n",
       "             (708, '歌'),\n",
       "             (709, '廃止'),\n",
       "             (710, '色'),\n",
       "             (711, '多数'),\n",
       "             (712, '##く'),\n",
       "             (713, 'タイトル'),\n",
       "             (714, '団体'),\n",
       "             (715, '##ガ'),\n",
       "             (716, '##キ'),\n",
       "             (717, '時点'),\n",
       "             (718, '２００２'),\n",
       "             (719, '当'),\n",
       "             (720, 'すべて'),\n",
       "             (721, 'しまう'),\n",
       "             (722, '##ｔ'),\n",
       "             (723, 'あるいは'),\n",
       "             (724, '条'),\n",
       "             (725, '##ブ'),\n",
       "             (726, '様々な'),\n",
       "             (727, '程度'),\n",
       "             (728, '小説'),\n",
       "             (729, '藩'),\n",
       "             (730, '記念'),\n",
       "             (731, '製'),\n",
       "             (732, '院'),\n",
       "             (733, '英語'),\n",
       "             (734, '参照'),\n",
       "             (735, '理由'),\n",
       "             (736, '大戦'),\n",
       "             (737, '持ち'),\n",
       "             (738, '##ｉｎ'),\n",
       "             (739, '事務'),\n",
       "             (740, '提供'),\n",
       "             (741, 'ド'),\n",
       "             (742, '導入'),\n",
       "             (743, '自動車'),\n",
       "             (744, '群'),\n",
       "             (745, 'Ｅ'),\n",
       "             (746, '１９９０'),\n",
       "             (747, 'ローマ'),\n",
       "             (748, '２０１８'),\n",
       "             (749, 'オ'),\n",
       "             (750, '##テ'),\n",
       "             (751, '死去'),\n",
       "             (752, '４０'),\n",
       "             (753, '人気'),\n",
       "             (754, '##ー'),\n",
       "             (755, 'たい'),\n",
       "             (756, '２００１'),\n",
       "             (757, '達'),\n",
       "             (758, '##ディ'),\n",
       "             (759, '労働'),\n",
       "             (760, '##島'),\n",
       "             (761, '生まれ'),\n",
       "             (762, 'よう'),\n",
       "             (763, '展開'),\n",
       "             (764, '##バ'),\n",
       "             (765, '周辺'),\n",
       "             (766, 'ザ'),\n",
       "             (767, '全体'),\n",
       "             (768, 'デザイン'),\n",
       "             (769, '運転'),\n",
       "             (770, '基'),\n",
       "             (771, '方法'),\n",
       "             (772, '領'),\n",
       "             (773, 'サービス'),\n",
       "             (774, '今'),\n",
       "             (775, '##ｅｎ'),\n",
       "             (776, 'その他'),\n",
       "             (777, 'サン'),\n",
       "             (778, '形式'),\n",
       "             (779, 'コ'),\n",
       "             (780, '真'),\n",
       "             (781, '公園'),\n",
       "             (782, '##ネ'),\n",
       "             (783, '橋'),\n",
       "             (784, '以来'),\n",
       "             (785, '##ｏｎ'),\n",
       "             (786, '白'),\n",
       "             (787, 'ラジオ'),\n",
       "             (788, '小学校'),\n",
       "             (789, 'サ'),\n",
       "             (790, 'しか'),\n",
       "             (791, '説'),\n",
       "             (792, '兵'),\n",
       "             (793, '結成'),\n",
       "             (794, '##一'),\n",
       "             (795, '議会'),\n",
       "             (796, '総合'),\n",
       "             (797, '同時に'),\n",
       "             (798, '大統領'),\n",
       "             (799, '##ａｌ'),\n",
       "             (800, 'のち'),\n",
       "             (801, '##人'),\n",
       "             (802, 'マン'),\n",
       "             (803, '##Ｌ'),\n",
       "             (804, '警察'),\n",
       "             (805, '舞台'),\n",
       "             (806, '環境'),\n",
       "             (807, '言わ'),\n",
       "             (808, '宇宙'),\n",
       "             (809, '支援'),\n",
       "             (810, '引退'),\n",
       "             (811, '##ｄ'),\n",
       "             (812, '娘'),\n",
       "             (813, '##グ'),\n",
       "             (814, '関連'),\n",
       "             (815, '中学校'),\n",
       "             (816, '最高'),\n",
       "             (817, '国民'),\n",
       "             (818, '由来'),\n",
       "             (819, '会議'),\n",
       "             (820, '初期'),\n",
       "             (821, 'スーパー'),\n",
       "             (822, '持って'),\n",
       "             (823, 'ほぼ'),\n",
       "             (824, '競技'),\n",
       "             (825, '自治'),\n",
       "             (826, '飛行'),\n",
       "             (827, '自ら'),\n",
       "             (828, '連邦'),\n",
       "             (829, 'バ'),\n",
       "             (830, 'スポーツ'),\n",
       "             (831, '##ｎ'),\n",
       "             (832, '完成'),\n",
       "             (833, 'いずれ'),\n",
       "             (834, '含ま'),\n",
       "             (835, '夏'),\n",
       "             (836, '北海道'),\n",
       "             (837, '裁判'),\n",
       "             (838, 'ドル'),\n",
       "             (839, '陸軍'),\n",
       "             (840, '##Ｔ'),\n",
       "             (841, '組'),\n",
       "             (842, '街'),\n",
       "             (843, '##Ｃ'),\n",
       "             (844, 'ひと'),\n",
       "             (845, '行い'),\n",
       "             (846, '＿'),\n",
       "             (847, 'カード'),\n",
       "             (848, '##ウ'),\n",
       "             (849, 'ｃｍ'),\n",
       "             (850, 'ル'),\n",
       "             (851, '変化'),\n",
       "             (852, '１９８０'),\n",
       "             (853, '巻'),\n",
       "             (854, '共同'),\n",
       "             (855, '特別'),\n",
       "             (856, '##か'),\n",
       "             (857, '寺'),\n",
       "             (858, '支配'),\n",
       "             (859, '花'),\n",
       "             (860, '士'),\n",
       "             (861, 'スペイン'),\n",
       "             (862, '主張'),\n",
       "             (863, '大正'),\n",
       "             (864, 'オリンピック'),\n",
       "             (865, '類'),\n",
       "             (866, '天'),\n",
       "             (867, '学者'),\n",
       "             (868, '##さ'),\n",
       "             (869, 'アン'),\n",
       "             (870, 'Ｗ'),\n",
       "             (871, '##み'),\n",
       "             (872, '対戦'),\n",
       "             (873, 'とって'),\n",
       "             (874, '##ａｎ'),\n",
       "             (875, '妻'),\n",
       "             (876, '丁目'),\n",
       "             (877, 'アジア'),\n",
       "             (878, 'パ'),\n",
       "             (879, '文'),\n",
       "             (880, '校'),\n",
       "             (881, '個'),\n",
       "             (882, '##ｅｓ'),\n",
       "             (883, '建築'),\n",
       "             (884, '##Ｂ'),\n",
       "             (885, '頭'),\n",
       "             (886, 'よく'),\n",
       "             (887, '整備'),\n",
       "             (888, '撮影'),\n",
       "             (889, '##ミ'),\n",
       "             (890, '形成'),\n",
       "             (891, '作戦'),\n",
       "             (892, '演奏'),\n",
       "             (893, '共和'),\n",
       "             (894, '##プ'),\n",
       "             (895, 'レ'),\n",
       "             (896, '英'),\n",
       "             (897, '神社'),\n",
       "             (898, '移動'),\n",
       "             (899, '右'),\n",
       "             (900, '輸送'),\n",
       "             (901, '与え'),\n",
       "             (902, '##Ｏ'),\n",
       "             (903, '##Ｅ'),\n",
       "             (904, '制度'),\n",
       "             (905, '１９９９'),\n",
       "             (906, 'ニューヨーク'),\n",
       "             (907, '法人'),\n",
       "             (908, '能力'),\n",
       "             (909, '発展'),\n",
       "             (910, '左'),\n",
       "             (911, '営業'),\n",
       "             (912, 'レース'),\n",
       "             (913, '非常に'),\n",
       "             (914, 'ハ'),\n",
       "             (915, '文字'),\n",
       "             (916, '登録'),\n",
       "             (917, '##マン'),\n",
       "             (918, '子供'),\n",
       "             (919, '状況'),\n",
       "             (920, '##木'),\n",
       "             (921, '伴い'),\n",
       "             (922, '準'),\n",
       "             (923, 'ス'),\n",
       "             (924, '風'),\n",
       "             (925, '表現'),\n",
       "             (926, '母'),\n",
       "             (927, '##Ｄ'),\n",
       "             (928, 'ファン'),\n",
       "             (929, '産業'),\n",
       "             (930, '韓国'),\n",
       "             (931, 'まま'),\n",
       "             (932, '表示'),\n",
       "             (933, 'イン'),\n",
       "             (934, '圏'),\n",
       "             (935, '企画'),\n",
       "             (936, '以後'),\n",
       "             (937, '試験'),\n",
       "             (938, '##ゴ'),\n",
       "             (939, '公式'),\n",
       "             (940, '着'),\n",
       "             (941, '得'),\n",
       "             (942, '##Ｆ'),\n",
       "             (943, '男'),\n",
       "             (944, 'データ'),\n",
       "             (945, '発行'),\n",
       "             (946, '行政'),\n",
       "             (947, '##トン'),\n",
       "             (948, '誌'),\n",
       "             (949, '区間'),\n",
       "             (950, '##す'),\n",
       "             (951, '##ラー'),\n",
       "             (952, '##デ'),\n",
       "             (953, '##Ｇ'),\n",
       "             (954, 'す'),\n",
       "             (955, '御'),\n",
       "             (956, '春'),\n",
       "             (957, '論'),\n",
       "             (958, '姿'),\n",
       "             (959, '合併'),\n",
       "             (960, '##Ｋ'),\n",
       "             (961, '距離'),\n",
       "             (962, '複数'),\n",
       "             (963, '心'),\n",
       "             (964, '##である'),\n",
       "             (965, '##ｕ'),\n",
       "             (966, '認め'),\n",
       "             (967, '新しい'),\n",
       "             (968, '方面'),\n",
       "             (969, '##ｍ'),\n",
       "             (970, '##スト'),\n",
       "             (971, '選出'),\n",
       "             (972, '分類'),\n",
       "             (973, '種類'),\n",
       "             (974, '広島'),\n",
       "             (975, '口'),\n",
       "             (976, '１９７０'),\n",
       "             (977, 'ｍｍ'),\n",
       "             (978, '##オ'),\n",
       "             (979, '階'),\n",
       "             (980, '新聞'),\n",
       "             (981, '##リン'),\n",
       "             (982, '赤'),\n",
       "             (983, 'シ'),\n",
       "             (984, '番号'),\n",
       "             (985, '##ｈ'),\n",
       "             (986, '成立'),\n",
       "             (987, '工業'),\n",
       "             (988, '##ｉｎｇ'),\n",
       "             (989, '##れ'),\n",
       "             (990, 'なければ'),\n",
       "             (991, '工場'),\n",
       "             (992, '運用'),\n",
       "             (993, '言語'),\n",
       "             (994, '自由'),\n",
       "             (995, '##ｌ'),\n",
       "             (996, '##ｃ'),\n",
       "             (997, '南部'),\n",
       "             (998, '装置'),\n",
       "             (999, '##ｋ'),\n",
       "             ...])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import BasicTokenizer\n",
    "\n",
    "\n",
    "class JumanTokenize(object):\n",
    "    \"\"\"Runs JumanTokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.juman = Juman()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = self.juman.analysis(text)\n",
    "        return [mrph.midasi for mrph in result.mrph_list()]\n",
    "    \n",
    "    # BasicTokenizerは以下をコメントアウト\n",
    "#text = self._tokenize_chinese_chars(text)  #漢字が全て一文字単位になってしまうのでコメントアウト。\n",
    "\n",
    "\n",
    "class BertTokenizer(object):\n",
    "    '''BERT用の文章の単語分割クラスを実装'''\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=False):   # 濁点が落ちるためFalseに設定（英語と異なる点）\n",
    "        '''\n",
    "        vocab_file：ボキャブラリーへのパス\n",
    "        do_lower_case：前処理で単語を小文字化するかどうか\n",
    "        '''\n",
    "\n",
    "        # ボキャブラリーのロード\n",
    "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
    "\n",
    "        # 分割処理の関数をフォルダ「utils」からimoprt、sub-wordで単語分割を行う\n",
    "        self.never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    "        # (注釈)上記の単語は途中で分割させない。これで一つの単語とみなす\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                              never_split=self.never_split)\n",
    "        self.juman_tokenizer = JumanTokenize()\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        '''文章を単語に分割する関数'''\n",
    "        split_tokens = []  # 分割後の単語たち\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            if token in self.never_split:\n",
    "                split_tokens.append(token)\n",
    "            else:\n",
    "                for sub_token in self.juman_tokenizer.tokenize(token):\n",
    "                    split_tokens.append(sub_token)    \n",
    "        return split_tokens\n",
    "    \n",
    "\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"分割された単語リストをIDに変換する関数\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                ids.append(self.vocab[token])\n",
    "            else:\n",
    "                ids.append(self.vocab[\"[UNK]\"])  #未知語の場合はＵＮＫ(=1)をセット\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"IDを単語に変換する関数\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bankの文脈による意味変化を単語ベクトルとして求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '自然', '言語', '処理', 'の', 'BERT', 'を', '勉強', '中', 'です', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"[CLS] 自然言語処理のBERTを勉強中です。 [SEP]\"\n",
    "\n",
    "# 単語分割Tokenizerを用意\n",
    "tokenizer = BertTokenizer(vocab_file=\"./vocab/vocab.txt\", do_lower_case=False)\n",
    "# 文章を単語分割\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# 確認\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1140, 993, 1536, 5, 1, 10, 6547, 68, 3338, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "# 単語をIDに変換する\n",
    "indexed_tokens= tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "print(indexed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '自然', '言語', '処理', 'の', '[UNK]', 'を', '勉強', '中', 'です', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#参考(ID->Tokenへ戻す)\n",
    "print([ids_to_tokens[i] for i in indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1140,  993, 1536,    5,    1,   10, 6547,   68, 3338,    7,    3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# リストをPyTorchのテンソルに\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 12, 12])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 12, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 12, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 12, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 12, 768])\n",
      "BertPooler:最終出力テンソル＝ torch.Size([1, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n",
      "encoded_layers.shape= torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# 文章をBERTで処理\n",
    "with torch.no_grad():\n",
    "    encoded_layers_1, _ = net(tokens_tensor, output_all_encoded_layers=True)  #12個の出力 [1, 12, 768]\n",
    "\n",
    "for layer in encoded_layers_1:\n",
    "    print(\"encoded_layers.shape=\",layer.shape)  # encoded_layers[0].shape= torch.Size([1, 12, 768])\n",
    "#12個ある。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskedWordPredictionsモジュールとSeqRelationshipモジュールの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    '''BERTの事前学習課題を行うアダプターモジュール'''\n",
    "\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "\n",
    "        # 事前学習課題：Masked Language Model用のモジュール\n",
    "        self.predictions = MaskedWordPredictions(config)\n",
    "\n",
    "        # 事前学習課題：Next Sentence Prediction用のモジュール\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        '''入力情報\n",
    "        sequence_output:[batch_size, seq_len, hidden_size]\n",
    "        pooled_output:[batch_size, hidden_size]\n",
    "        '''\n",
    "        # 入力のマスクされた各単語がどの単語かを判定\n",
    "        # 出力 [minibatch, seq_len, vocab_size]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定\n",
    "        seq_relationship_score = self.seq_relationship(\n",
    "            pooled_output)  # 出力 [minibatch, 2]\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BERTの事前学習課題を行うアダプターモジュール'''\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "\n",
    "        # 事前学習課題：Masked Language Model用のモジュール\n",
    "        self.predictions = MaskedWordPredictions(config)\n",
    "\n",
    "        # 事前学習課題：Next Sentence Prediction用のモジュール\n",
    "        self.seq_relationship = SeqRelationship(config, out_features=2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        '''入力情報\n",
    "        sequence_output:[batch_size, seq_len, hidden_size]\n",
    "        pooled_output:[batch_size, hidden_size]\n",
    "        '''\n",
    "        # 入力のマスクされた各単語がどの単語かを判定\n",
    "        # 出力 [batch_size, seq_len, hidden_size]\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定\n",
    "        seq_relationship_score = self.seq_relationship(\n",
    "            pooled_output)  # 出力 [batch_size, 2]\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前学習課題：Masked Language Model用のモジュール\n",
    "\n",
    "\n",
    "class MaskedWordPredictions(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''事前学習課題：Masked Language Model用のモジュール\n",
    "        元の[2]の実装では、BertLMPredictionHeadという名前です。\n",
    "        '''\n",
    "        super(MaskedWordPredictions, self).__init__()\n",
    "\n",
    "        # BERTから出力された特徴量を変換するモジュール（入出力のサイズは同じ）\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # self.transformの出力から、各位置の単語がどれかを当てる全結合層\n",
    "        self.decoder = nn.Linear(in_features=config.hidden_size,  # 'hidden_size': 768\n",
    "                                 out_features=config.vocab_size,  # 'vocab_size': 30522\n",
    "                                 bias=False)\n",
    "        # バイアス項\n",
    "        self.bias = nn.Parameter(torch.zeros(\n",
    "            config.vocab_size))  # 'vocab_size': 30522\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states：BERTからの出力[batch_size, seq_len, hidden_size]\n",
    "        '''\n",
    "        # BERTから出力された特徴量を変換\n",
    "        # 出力サイズ：[batch_size, seq_len, hidden_size]\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "\n",
    "        # 各位置の単語がボキャブラリーのどの単語なのかをクラス分類で予測\n",
    "        # 出力サイズ：[batch_size, seq_len, vocab_size]\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    '''MaskedWordPredictionsにて、BERTからの特徴量を変換するモジュール（入出力のサイズは同じ）'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertPredictionHeadTransform, self).__init__()\n",
    "\n",
    "        # 全結合層 'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        # 活性化関数gelu\n",
    "        self.transform_act_fn = gelu\n",
    "\n",
    "        # LayerNormalization\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        '''hidden_statesはsequence_output:[minibatch, seq_len, hidden_size]'''\n",
    "        # 全結合層で特徴量変換し、活性化関数geluを計算したあと、LayerNormalizationする\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前学習課題：Next Sentence Prediction用のモジュール\n",
    "class SeqRelationship(nn.Module):\n",
    "    def __init__(self, config, out_features):\n",
    "        '''事前学習課題：Next Sentence Prediction用のモジュール\n",
    "        元の引用[2]の実装では、とくにクラスとして用意はしていない。\n",
    "        ただの全結合層に、わざわざ名前をつけた。\n",
    "        '''\n",
    "        super(SeqRelationship, self).__init__()\n",
    "\n",
    "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定するクラス分類の全結合層\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, out_features)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        return self.seq_relationship(pooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(nn.Module):\n",
    "    '''BERTモデルに、事前学習課題用のアダプターモジュール\n",
    "    BertPreTrainingHeadsをつなげたモデル'''\n",
    "\n",
    "    def __init__(self, config, net_bert):\n",
    "        super(BertForMaskedLM, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "\n",
    "        # 事前学習課題用のアダプターモジュール\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        encoded_layers, pooled_output = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "        # 事前学習課題の推論を実施\n",
    "        prediction_scores, seq_relationship_score = self.cls(\n",
    "            encoded_layers, pooled_output)\n",
    "\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルのロード部分を実装します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight→bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight→bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight→bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight→bert.embeddings.LayerNorm.gamma\n",
      "bert.embeddings.LayerNorm.bias→bert.embeddings.LayerNorm.beta\n",
      "bert.encoder.layer.0.attention.self.query.weight→bert.encoder.layer.0.attention.selfattn.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias→bert.encoder.layer.0.attention.selfattn.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight→bert.encoder.layer.0.attention.selfattn.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias→bert.encoder.layer.0.attention.selfattn.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight→bert.encoder.layer.0.attention.selfattn.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias→bert.encoder.layer.0.attention.selfattn.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight→bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias→bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight→bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias→bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.0.intermediate.dense.weight→bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias→bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight→bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias→bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight→bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.output.LayerNorm.bias→bert.encoder.layer.0.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.attention.self.query.weight→bert.encoder.layer.1.attention.selfattn.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias→bert.encoder.layer.1.attention.selfattn.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight→bert.encoder.layer.1.attention.selfattn.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias→bert.encoder.layer.1.attention.selfattn.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight→bert.encoder.layer.1.attention.selfattn.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias→bert.encoder.layer.1.attention.selfattn.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight→bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias→bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight→bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias→bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.intermediate.dense.weight→bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias→bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight→bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias→bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight→bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.output.LayerNorm.bias→bert.encoder.layer.1.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.attention.self.query.weight→bert.encoder.layer.2.attention.selfattn.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias→bert.encoder.layer.2.attention.selfattn.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight→bert.encoder.layer.2.attention.selfattn.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias→bert.encoder.layer.2.attention.selfattn.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight→bert.encoder.layer.2.attention.selfattn.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias→bert.encoder.layer.2.attention.selfattn.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight→bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias→bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight→bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias→bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.intermediate.dense.weight→bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias→bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight→bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias→bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight→bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.output.LayerNorm.bias→bert.encoder.layer.2.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.attention.self.query.weight→bert.encoder.layer.3.attention.selfattn.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias→bert.encoder.layer.3.attention.selfattn.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight→bert.encoder.layer.3.attention.selfattn.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias→bert.encoder.layer.3.attention.selfattn.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight→bert.encoder.layer.3.attention.selfattn.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias→bert.encoder.layer.3.attention.selfattn.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight→bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias→bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight→bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias→bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.intermediate.dense.weight→bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias→bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight→bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias→bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight→bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.output.LayerNorm.bias→bert.encoder.layer.3.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.attention.self.query.weight→bert.encoder.layer.4.attention.selfattn.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias→bert.encoder.layer.4.attention.selfattn.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight→bert.encoder.layer.4.attention.selfattn.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias→bert.encoder.layer.4.attention.selfattn.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight→bert.encoder.layer.4.attention.selfattn.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias→bert.encoder.layer.4.attention.selfattn.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight→bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias→bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight→bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias→bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.intermediate.dense.weight→bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias→bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight→bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias→bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight→bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.output.LayerNorm.bias→bert.encoder.layer.4.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.attention.self.query.weight→bert.encoder.layer.5.attention.selfattn.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias→bert.encoder.layer.5.attention.selfattn.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight→bert.encoder.layer.5.attention.selfattn.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias→bert.encoder.layer.5.attention.selfattn.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight→bert.encoder.layer.5.attention.selfattn.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias→bert.encoder.layer.5.attention.selfattn.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight→bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias→bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight→bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias→bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.intermediate.dense.weight→bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias→bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight→bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias→bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight→bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.output.LayerNorm.bias→bert.encoder.layer.5.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.attention.self.query.weight→bert.encoder.layer.6.attention.selfattn.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias→bert.encoder.layer.6.attention.selfattn.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight→bert.encoder.layer.6.attention.selfattn.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias→bert.encoder.layer.6.attention.selfattn.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight→bert.encoder.layer.6.attention.selfattn.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias→bert.encoder.layer.6.attention.selfattn.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight→bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias→bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight→bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias→bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.intermediate.dense.weight→bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias→bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight→bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias→bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight→bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.output.LayerNorm.bias→bert.encoder.layer.6.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.attention.self.query.weight→bert.encoder.layer.7.attention.selfattn.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias→bert.encoder.layer.7.attention.selfattn.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight→bert.encoder.layer.7.attention.selfattn.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias→bert.encoder.layer.7.attention.selfattn.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight→bert.encoder.layer.7.attention.selfattn.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias→bert.encoder.layer.7.attention.selfattn.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight→bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias→bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight→bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias→bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.intermediate.dense.weight→bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias→bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight→bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias→bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight→bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.output.LayerNorm.bias→bert.encoder.layer.7.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.attention.self.query.weight→bert.encoder.layer.8.attention.selfattn.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias→bert.encoder.layer.8.attention.selfattn.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight→bert.encoder.layer.8.attention.selfattn.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias→bert.encoder.layer.8.attention.selfattn.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight→bert.encoder.layer.8.attention.selfattn.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias→bert.encoder.layer.8.attention.selfattn.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight→bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias→bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight→bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias→bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.intermediate.dense.weight→bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias→bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight→bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias→bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight→bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.output.LayerNorm.bias→bert.encoder.layer.8.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.attention.self.query.weight→bert.encoder.layer.9.attention.selfattn.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias→bert.encoder.layer.9.attention.selfattn.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight→bert.encoder.layer.9.attention.selfattn.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias→bert.encoder.layer.9.attention.selfattn.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight→bert.encoder.layer.9.attention.selfattn.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias→bert.encoder.layer.9.attention.selfattn.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight→bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias→bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight→bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias→bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.intermediate.dense.weight→bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias→bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight→bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias→bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight→bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.output.LayerNorm.bias→bert.encoder.layer.9.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.attention.self.query.weight→bert.encoder.layer.10.attention.selfattn.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias→bert.encoder.layer.10.attention.selfattn.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight→bert.encoder.layer.10.attention.selfattn.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias→bert.encoder.layer.10.attention.selfattn.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight→bert.encoder.layer.10.attention.selfattn.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias→bert.encoder.layer.10.attention.selfattn.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight→bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias→bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight→bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias→bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.intermediate.dense.weight→bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias→bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight→bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias→bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight→bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.output.LayerNorm.bias→bert.encoder.layer.10.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.attention.self.query.weight→bert.encoder.layer.11.attention.selfattn.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias→bert.encoder.layer.11.attention.selfattn.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight→bert.encoder.layer.11.attention.selfattn.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias→bert.encoder.layer.11.attention.selfattn.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight→bert.encoder.layer.11.attention.selfattn.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias→bert.encoder.layer.11.attention.selfattn.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight→bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias→bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight→bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias→bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.intermediate.dense.weight→bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias→bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight→bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias→bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight→bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.output.LayerNorm.bias→bert.encoder.layer.11.output.LayerNorm.beta\n",
      "bert.pooler.dense.weight→bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias→bert.pooler.dense.bias\n",
      "cls.predictions.bias→cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight→cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias→cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight→cls.predictions.transform.LayerNorm.gamma\n",
      "cls.predictions.transform.LayerNorm.bias→cls.predictions.transform.LayerNorm.beta\n",
      "cls.predictions.decoder.weight→cls.predictions.decoder.weight\n",
      "cls.seq_relationship.weight→cls.seq_relationship.seq_relationship.weight\n",
      "cls.seq_relationship.bias→cls.seq_relationship.seq_relationship.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTの基本モデル\n",
    "net_bert = BertModel(config)\n",
    "net_bert.eval()\n",
    "\n",
    "# 事前学習課題のアダプターモジュールを搭載したBERT\n",
    "net = BertForMaskedLM(config, net_bert)\n",
    "net.eval()\n",
    "\n",
    "# 学習済みの重みをロード\n",
    "weights_path = \"./weights/pytorch_model.bin\"\n",
    "loaded_state_dict = torch.load(weights_path)\n",
    "\n",
    "# 現在のネットワークモデルのパラメータ名\n",
    "param_names = []  # パラメータの名前を格納していく\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param_names.append(name)\n",
    "\n",
    "# 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
    "new_state_dict = net.state_dict().copy()\n",
    "\n",
    "# 新たなstate_dictに学習済みの値を代入\n",
    "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
    "    name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
    "    new_state_dict[name] = value  # 値を入れる\n",
    "    print(str(key_name)+\"→\"+str(name))  # 何から何に入ったかを表示\n",
    "        # 現在のネットワークのパラメータを全部ロードしたら終える\n",
    "    if index+1 >= len(param_names):\n",
    "        break\n",
    "        \n",
    "# 新たなstate_dictを構築したBERTモデルに与える\n",
    "net.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習課題Masked Language Modelを試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '今日', 'の', '天気', 'は', '雨', 'てす', '。', '。', '[SEP]', '明日', 'の', '天気', 'は', '晴', 'てす', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 入力する文章を用意\n",
    "text = \"[CLS] 今日の天気は雨です。。 [SEP] 明日の天気は晴です。 [SEP] \"\n",
    "\n",
    "# 単語分割Tokenizerを用意\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=\"./vocab/vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# 文章を単語分割\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '今日', 'の', '[MASK]', 'は', '雨', 'てす', '。', '。', '[SEP]', '明日', 'の', '天気', 'は', '晴', 'てす', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 単語をマスクする。今回は7単語目の「勉強をマスクして当てさせる\n",
    "masked_index = 3\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "print(tokenized_text)  # 13単語目が[MASK]になっている\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2281, 5, 4, 9, 2899, 1, 7, 7, 3, 12072, 5, 9292, 9, 4784, 1, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "# 単語をIDに変換する\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 1文目に0を2文目に1を入れた文章IDを用意\n",
    "\n",
    "\n",
    "def seq2id(indexed_tokens):\n",
    "    '''分かち書きされた単語ID列を文章IDに。[SEP]で分ける'''\n",
    "\n",
    "    segments_ids = []\n",
    "    seq_id = 0\n",
    "\n",
    "    for word_id in indexed_tokens:\n",
    "        segments_ids.append(seq_id)  # seq_id=o or 1を追加\n",
    "\n",
    "        # [SEP]を発見したら2文目になるので以降idを1に\n",
    "        if word_id == 3:  # IDの3が[SEP]である\n",
    "            seq_id = 1\n",
    "\n",
    "    return segments_ids\n",
    "\n",
    "\n",
    "# 実行\n",
    "segments_ids = seq2id(indexed_tokens)\n",
    "print(segments_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  2281,     5,     4,     9,  2899,     1,     7,     7,     3,\n",
      "         12072,     5,  9292,     9,  4784,     1,     7,     3]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfAttention:attention_scoresテンソル＝ torch.Size([1, 12, 18, 18])\n",
      "BertSelfAttention: context_layer元のテンソルに戻した後のテンソル＝ torch.Size([1, 18, 768])\n",
      "BertSelfOutputの出力テンソル＝ torch.Size([1, 18, 768])\n",
      "BertIntermediateの出力テンソル＝ torch.Size([1, 18, 3072])\n",
      "BertOutput:hidden_statesのテンソル＝ torch.Size([1, 18, 768])\n",
      "BertPooler:最終出力テンソル＝ torch.Size([1, 768])\n",
      "torch.Size([1, 18, 32006])\n",
      "torch.Size([1, 2])\n",
      "tensor([[ 3.9003, -3.9128]])\n"
     ]
    }
   ],
   "source": [
    "# モデルで推論\n",
    "\n",
    "# リストをPyTorchのテンソルにしてモデルに入力\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "print(tokens_tensor)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(segments_tensors)\n",
    "\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    prediction_scores, seq_relationship_score = net(\n",
    "        tokens_tensor, segments_tensors)\n",
    "\n",
    "print(prediction_scores.shape)\n",
    "print(seq_relationship_score.shape)\n",
    "print(seq_relationship_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天気\n"
     ]
    }
   ],
   "source": [
    "# 推論したIDを単語に戻す\n",
    "predicted_index = torch.argmax(prediction_scores[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
