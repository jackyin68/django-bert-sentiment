{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 BERTを用いたレビュー文章に対する感情分析モデルの実装と学習・推論\n",
    "\n",
    "本ファイルでは、BERTを使用し、IMDbデータのポジ・ネガを分類するモデルを学習させ、推論します。また推論時のSelf-Attentionを可視化します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 学習目標\n",
    "\n",
    "1.\tBERTのボキャブラリーをtorchtextで使用する実装方法を理解する\n",
    "2.\tBERTに分類タスク用のアダプターモジュールを追加し、感情分析を実施するモデルを実装できる\n",
    "3.\tBERTをファインチューニングして、モデルを学習できる\n",
    "4.  BERTのSelf-Attentionの重みを可視化し、推論の説明を試みることができる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パスの追加\n",
    "import sys\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python36.zip')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/lib-dynload')\n",
    "sys.path.append('/home/siny/.local/lib/python3.6/site-packages')\n",
    "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "\n",
    "- 書籍の指示に従い、本章で使用するデータを用意します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDbデータを読み込み、DataLoaderを作成（BERTのTokenizerを使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理と単語分割をまとめた関数を作成\n",
    "import re\n",
    "import string\n",
    "import mojimoji\n",
    "from utils.bert import BertTokenizer,BertForIMDb\n",
    "# フォルダ「utils」のbert.pyより\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert = BertTokenizer(\n",
    "    vocab_file=\"./vocab/vocab.txt\", do_lower_case=False)\n",
    "\n",
    "def replace_unk(text):\n",
    "    tokens = []\n",
    "\n",
    "# 前処理と単語分割をまとめた関数を定義\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)  # tokenizer_bert\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=False, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"[CLS]\", eos_token=\"[SEP]\", pad_token='[PAD]', unk_token='[UNK]')\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# (注釈)：各引数を再確認\n",
    "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
    "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
    "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
    "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
    "# include_length: 文章の単語数のデータを保持するか\n",
    "# batch_first：ミニバッチの次元を先頭に用意するかどうか\n",
    "# fix_length：全部の文章を指定した長さと同じになるように、paddingします\n",
    "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "# BERT用で処理するので、10分弱時間がかかります\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='train.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "#train_ds, val_ds = train_val_ds.split(\n",
    "#    split_ratio=0.8, random_state=random.seed(1234))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTはBERTが持つ全単語でBertEmbeddingモジュールを作成しているので、ボキャブラリーとしては全単語を使用します\n",
    "# そのため訓練データからボキャブラリーは作成しません\n",
    "\n",
    "# まずBERT用の単語辞書を辞書型変数に用意します\n",
    "from utils.bert import BertTokenizer, load_vocab\n",
    "\n",
    "vocab_bert, ids_to_tokens_bert = load_vocab(\n",
    "    vocab_file=\"./vocab/vocab.txt\")\n",
    "\n",
    "\n",
    "# このまま、TEXT.vocab.stoi= vocab_bert (stoiはstring_to_IDで、単語からIDへの辞書)としたいですが、\n",
    "# 一度bulild_vocabを実行しないとTEXTオブジェクトがvocabのメンバ変数をもってくれないです。\n",
    "# （'Field' object has no attribute 'vocab' というエラーをはきます）\n",
    "\n",
    "# 1度適当にbuild_vocabでボキャブラリーを作成してから、BERTのボキャブラリーを上書きします\n",
    "TEXT.build_vocab(train_val_ds, min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_path = '/mnt/c/Users/sinfo/Desktop/pytorch/pytorch_advanced-master/django/bert/app1/data/text.pkl'\n",
    "def pickle_dump(TEXT, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(TEXT, f)\n",
    "        \n",
    "pickle_dump(TEXT, pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXTオブジェクトのロード\n",
    "import pickle\n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        TEXT = pickle.load(f)\n",
    "    return TEXT\n",
    "pkl_path = '/mnt/c/Users/sinfo/Desktop/pytorch/pytorch_advanced-master/django/bert/app1/data/text.pkl'\n",
    "TEXT = pickle_load(pkl_path)   #vocabデータのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'[PAD]': 0,\n",
       "             '[UNK]': 1,\n",
       "             '[CLS]': 2,\n",
       "             '[SEP]': 3,\n",
       "             '[MASK]': 4,\n",
       "             'の': 5,\n",
       "             '、': 6,\n",
       "             '。': 7,\n",
       "             'に': 8,\n",
       "             'は': 9,\n",
       "             'を': 10,\n",
       "             'が': 11,\n",
       "             'と': 12,\n",
       "             'で': 13,\n",
       "             '年': 14,\n",
       "             '・': 15,\n",
       "             '（': 16,\n",
       "             '）': 17,\n",
       "             'さ': 18,\n",
       "             'して': 19,\n",
       "             'した': 20,\n",
       "             'いる': 21,\n",
       "             'する': 22,\n",
       "             'も': 23,\n",
       "             '「': 24,\n",
       "             '」': 25,\n",
       "             '月': 26,\n",
       "             'から': 27,\n",
       "             'れた': 28,\n",
       "             '日': 29,\n",
       "             'こと': 30,\n",
       "             'し': 31,\n",
       "             'である': 32,\n",
       "             'れて': 33,\n",
       "             'や': 34,\n",
       "             '２': 35,\n",
       "             '１': 36,\n",
       "             'いた': 37,\n",
       "             'ある': 38,\n",
       "             '『': 39,\n",
       "             '』': 40,\n",
       "             'れる': 41,\n",
       "             'など': 42,\n",
       "             '３': 43,\n",
       "             '−': 44,\n",
       "             '': 2450,\n",
       "             'この': 46,\n",
       "             'ない': 47,\n",
       "             'ため': 48,\n",
       "             '日本': 49,\n",
       "             '人': 50,\n",
       "             '”': 51,\n",
       "             'より': 52,\n",
       "             '４': 53,\n",
       "             'れ': 54,\n",
       "             '第': 55,\n",
       "             'いう': 56,\n",
       "             '者': 57,\n",
       "             'その': 58,\n",
       "             'なった': 59,\n",
       "             'もの': 60,\n",
       "             'へ': 61,\n",
       "             '後': 62,\n",
       "             'まで': 63,\n",
       "             'また': 64,\n",
       "             '市': 65,\n",
       "             'なる': 66,\n",
       "             '５': 67,\n",
       "             '中': 68,\n",
       "             '６': 69,\n",
       "             '一': 70,\n",
       "             '同': 71,\n",
       "             '県': 72,\n",
       "             'これ': 73,\n",
       "             '１０': 74,\n",
       "             '７': 75,\n",
       "             '内': 76,\n",
       "             '８': 77,\n",
       "             'なって': 78,\n",
       "             'おり': 79,\n",
       "             'よる': 80,\n",
       "             '９': 81,\n",
       "             '大学': 82,\n",
       "             'つ': 83,\n",
       "             '大': 84,\n",
       "             '国': 85,\n",
       "             'よって': 86,\n",
       "             '時': 87,\n",
       "             '１２': 88,\n",
       "             'であった': 89,\n",
       "             'か': 90,\n",
       "             '家': 91,\n",
       "             '駅': 92,\n",
       "             'ように': 93,\n",
       "             'ら': 94,\n",
       "             '現在': 95,\n",
       "             '的な': 96,\n",
       "             '本': 97,\n",
       "             '１１': 98,\n",
       "             '軍': 99,\n",
       "             '上': 100,\n",
       "             '：': 101,\n",
       "             '化': 102,\n",
       "             'であり': 103,\n",
       "             '的に': 104,\n",
       "             'なり': 105,\n",
       "             '放送': 106,\n",
       "             '名': 107,\n",
       "             '性': 108,\n",
       "             'ず': 109,\n",
       "             '部': 110,\n",
       "             '回': 111,\n",
       "             '目': 112,\n",
       "             '町': 113,\n",
       "             '時代': 114,\n",
       "             'それ': 115,\n",
       "             'なかった': 116,\n",
       "             'おいて': 117,\n",
       "             '世界': 118,\n",
       "             '代': 119,\n",
       "             '線': 120,\n",
       "             '間': 121,\n",
       "             '戦': 122,\n",
       "             'でも': 123,\n",
       "             'られる': 124,\n",
       "             'あり': 125,\n",
       "             '会': 126,\n",
       "             '場合': 127,\n",
       "             '行わ': 128,\n",
       "             '二': 129,\n",
       "             'ついて': 130,\n",
       "             '所': 131,\n",
       "             'その後': 132,\n",
       "             '東京': 133,\n",
       "             '前': 134,\n",
       "             '多く': 135,\n",
       "             '州': 136,\n",
       "             'だった': 137,\n",
       "             '地': 138,\n",
       "             'あった': 139,\n",
       "             'なく': 140,\n",
       "             'しかし': 141,\n",
       "             'い': 142,\n",
       "             'られた': 143,\n",
       "             '号': 144,\n",
       "             '数': 145,\n",
       "             'できる': 146,\n",
       "             '的': 147,\n",
       "             '作品': 148,\n",
       "             '彼': 149,\n",
       "             '選手': 150,\n",
       "             '他': 151,\n",
       "             '｜': 152,\n",
       "             '使用': 153,\n",
       "             '機': 154,\n",
       "             '昭和': 155,\n",
       "             '語': 156,\n",
       "             'られて': 157,\n",
       "             '郡': 158,\n",
       "             '位': 159,\n",
       "             '研究': 160,\n",
       "             '当時': 161,\n",
       "             '存在': 162,\n",
       "             '新': 163,\n",
       "             '元': 164,\n",
       "             'アメリカ': 165,\n",
       "             '長': 166,\n",
       "             '側': 167,\n",
       "             '三': 168,\n",
       "             '活動': 169,\n",
       "             '映画': 170,\n",
       "             '初': 171,\n",
       "             '学校': 172,\n",
       "             '社': 173,\n",
       "             '等': 174,\n",
       "             '１５': 175,\n",
       "             '，': 176,\n",
       "             '全': 177,\n",
       "             '下': 178,\n",
       "             '番組': 179,\n",
       "             '呼ば': 180,\n",
       "             '東': 181,\n",
       "             '区': 182,\n",
       "             '２０': 183,\n",
       "             '会社': 184,\n",
       "             '出身': 185,\n",
       "             'および': 186,\n",
       "             '／': 187,\n",
       "             '車': 188,\n",
       "             '約': 189,\n",
       "             'のみ': 190,\n",
       "             '代表': 191,\n",
       "             '形': 192,\n",
       "             '権': 193,\n",
       "             'なお': 194,\n",
       "             'テレビ': 195,\n",
       "             '西': 196,\n",
       "             '系': 197,\n",
       "             '発売': 198,\n",
       "             '型': 199,\n",
       "             '以下': 200,\n",
       "             '地域': 201,\n",
       "             '法': 202,\n",
       "             '開発': 203,\n",
       "             '１４': 204,\n",
       "             '歳': 205,\n",
       "             '作': 206,\n",
       "             '１３': 207,\n",
       "             '１６': 208,\n",
       "             '中心': 209,\n",
       "             'チーム': 210,\n",
       "             'たち': 211,\n",
       "             '北': 212,\n",
       "             '分': 213,\n",
       "             'られ': 214,\n",
       "             '館': 215,\n",
       "             '鉄道': 216,\n",
       "             'おける': 217,\n",
       "             '時間': 218,\n",
       "             '以降': 219,\n",
       "             '３０': 220,\n",
       "             'ドイツ': 221,\n",
       "             '小': 222,\n",
       "             '出場': 223,\n",
       "             '一部': 224,\n",
       "             '南': 225,\n",
       "             '用': 226,\n",
       "             'さらに': 227,\n",
       "             '！': 228,\n",
       "             '発表': 229,\n",
       "             '度': 230,\n",
       "             '試合': 231,\n",
       "             '平成': 232,\n",
       "             '＝': 233,\n",
       "             'だ': 234,\n",
       "             '高': 235,\n",
       "             '学': 236,\n",
       "             '賞': 237,\n",
       "             '局': 238,\n",
       "             '登場': 239,\n",
       "             '大会': 240,\n",
       "             '版': 241,\n",
       "             '開始': 242,\n",
       "             '（）': 243,\n",
       "             '次': 244,\n",
       "             'フランス': 245,\n",
       "             '川': 246,\n",
       "             '際': 247,\n",
       "             '点': 248,\n",
       "             '式': 249,\n",
       "             '関係': 250,\n",
       "             '曲': 251,\n",
       "             '参加': 252,\n",
       "             '記録': 253,\n",
       "             '体': 254,\n",
       "             'ような': 255,\n",
       "             '所属': 256,\n",
       "             '％': 257,\n",
       "             '多い': 258,\n",
       "             '利用': 259,\n",
       "             'ｍ': 260,\n",
       "             'でき': 261,\n",
       "             'だけ': 262,\n",
       "             '世': 263,\n",
       "             '．': 264,\n",
       "             '１８': 265,\n",
       "             '１７': 266,\n",
       "             'シリーズ': 267,\n",
       "             '明治': 268,\n",
       "             '以上': 269,\n",
       "             '事': 270,\n",
       "             'ゲーム': 271,\n",
       "             '見': 272,\n",
       "             'お': 273,\n",
       "             '力': 274,\n",
       "             '##な': 275,\n",
       "             'な': 276,\n",
       "             '音楽': 277,\n",
       "             'せ': 278,\n",
       "             'シーズン': 279,\n",
       "             '開催': 280,\n",
       "             '##子': 281,\n",
       "             'リーグ': 282,\n",
       "             '島': 283,\n",
       "             'ともに': 284,\n",
       "             'にて': 285,\n",
       "             '各': 286,\n",
       "             '級': 287,\n",
       "             '国際': 288,\n",
       "             'いった': 289,\n",
       "             '監督': 290,\n",
       "             '氏': 291,\n",
       "             'イギリス': 292,\n",
       "             '山': 293,\n",
       "             '両': 294,\n",
       "             '世紀': 295,\n",
       "             '問題': 296,\n",
       "             '村': 297,\n",
       "             '２０１０': 298,\n",
       "             '旧': 299,\n",
       "             '対して': 300,\n",
       "             '優勝': 301,\n",
       "             '知ら': 302,\n",
       "             '都市': 303,\n",
       "             '１９': 304,\n",
       "             '行う': 305,\n",
       "             '金': 306,\n",
       "             '場': 307,\n",
       "             '道': 308,\n",
       "             '一般': 309,\n",
       "             '中国': 310,\n",
       "             '物': 311,\n",
       "             '出演': 312,\n",
       "             '設置': 313,\n",
       "             'ので': 314,\n",
       "             'せる': 315,\n",
       "             '持つ': 316,\n",
       "             '地方': 317,\n",
       "             '事業': 318,\n",
       "             '社会': 319,\n",
       "             '卒業': 320,\n",
       "             '戦争': 321,\n",
       "             '共に': 322,\n",
       "             '官': 323,\n",
       "             '委員': 324,\n",
       "             '２５': 325,\n",
       "             '位置': 326,\n",
       "             '計画': 327,\n",
       "             '同じ': 328,\n",
       "             'Ｂ': 329,\n",
       "             'プロ': 330,\n",
       "             '２００９': 331,\n",
       "             '変更': 332,\n",
       "             'Ａ': 333,\n",
       "             '２００８': 334,\n",
       "             '省': 335,\n",
       "             '教育': 336,\n",
       "             '結果': 337,\n",
       "             '中央': 338,\n",
       "             '２０１１': 339,\n",
       "             '大阪': 340,\n",
       "             '２００７': 341,\n",
       "             '王': 342,\n",
       "             '影響': 343,\n",
       "             'アルバム': 344,\n",
       "             '２１': 345,\n",
       "             '期': 346,\n",
       "             '文化': 347,\n",
       "             'きた': 348,\n",
       "             '頃': 349,\n",
       "             '政府': 350,\n",
       "             '隊': 351,\n",
       "             '役': 352,\n",
       "             'ほか': 353,\n",
       "             '番': 354,\n",
       "             'そして': 355,\n",
       "             '２０１２': 356,\n",
       "             '特に': 357,\n",
       "             '獲得': 358,\n",
       "             'ながら': 359,\n",
       "             'Ｃ': 360,\n",
       "             '##に': 361,\n",
       "             '派': 362,\n",
       "             '一方': 363,\n",
       "             '手': 364,\n",
       "             '建設': 365,\n",
       "             '２４': 366,\n",
       "             'いく': 367,\n",
       "             '通り': 368,\n",
       "             '##ラ': 369,\n",
       "             '##ａ': 370,\n",
       "             'なら': 371,\n",
       "             'とき': 372,\n",
       "             '自身': 373,\n",
       "             '担当': 374,\n",
       "             '外': 375,\n",
       "             '考え': 376,\n",
       "             '２００６': 377,\n",
       "             '当初': 378,\n",
       "             'ｋｍ': 379,\n",
       "             '##ク': 380,\n",
       "             'うち': 381,\n",
       "             '設立': 382,\n",
       "             '方': 383,\n",
       "             'バス': 384,\n",
       "             '##ｓ': 385,\n",
       "             '機関': 386,\n",
       "             '円': 387,\n",
       "             '及び': 388,\n",
       "             '種': 389,\n",
       "             '事件': 390,\n",
       "             '##ナ': 391,\n",
       "             '経済': 392,\n",
       "             '２２': 393,\n",
       "             '２０１３': 394,\n",
       "             '現': 395,\n",
       "             '生': 396,\n",
       "             '年間': 397,\n",
       "             '２０１４': 398,\n",
       "             'ア': 399,\n",
       "             'かけて': 400,\n",
       "             '０': 401,\n",
       "             '情報': 402,\n",
       "             '選挙': 403,\n",
       "             '水': 404,\n",
       "             '女性': 405,\n",
       "             '都': 406,\n",
       "             '政治': 407,\n",
       "             '主義': 408,\n",
       "             '販売': 409,\n",
       "             'それぞれ': 410,\n",
       "             '２３': 411,\n",
       "             '意味': 412,\n",
       "             '城': 413,\n",
       "             '以外': 414,\n",
       "             '地区': 415,\n",
       "             '不': 416,\n",
       "             '攻撃': 417,\n",
       "             '２０１５': 418,\n",
       "             '科': 419,\n",
       "             'デビュー': 420,\n",
       "             '##ｅ': 421,\n",
       "             '##の': 422,\n",
       "             'Ｓ': 423,\n",
       "             '子': 424,\n",
       "             '調査': 425,\n",
       "             '艦': 426,\n",
       "             '構成': 427,\n",
       "             'グループ': 428,\n",
       "             '##で': 429,\n",
       "             'Ｄ': 430,\n",
       "             '高等': 431,\n",
       "             '米': 432,\n",
       "             'かつて': 433,\n",
       "             '２０１６': 434,\n",
       "             '再': 435,\n",
       "             '発生': 436,\n",
       "             '##タ': 437,\n",
       "             '２００５': 438,\n",
       "             'アメリカ合衆国': 439,\n",
       "             '##し': 440,\n",
       "             '受け': 441,\n",
       "             '技術': 442,\n",
       "             '##ノ': 443,\n",
       "             '##山': 444,\n",
       "             '##り': 445,\n",
       "             '可能': 446,\n",
       "             '自分': 447,\n",
       "             '##ｉ': 448,\n",
       "             'または': 449,\n",
       "             '航空': 450,\n",
       "             '就任': 451,\n",
       "             '父': 452,\n",
       "             '##ｙ': 453,\n",
       "             '##ｅｒ': 454,\n",
       "             '２０００': 455,\n",
       "             '路線': 456,\n",
       "             '状態': 457,\n",
       "             '##ｏ': 458,\n",
       "             '歴史': 459,\n",
       "             '話': 460,\n",
       "             'システム': 461,\n",
       "             '行って': 462,\n",
       "             '場所': 463,\n",
       "             '##ス': 464,\n",
       "             '勝': 465,\n",
       "             '採用': 466,\n",
       "             '２６': 467,\n",
       "             '行った': 468,\n",
       "             '決定': 469,\n",
       "             '##田': 470,\n",
       "             '契約': 471,\n",
       "             '２８': 472,\n",
       "             'Ｍ': 473,\n",
       "             '神': 474,\n",
       "             '２０１７': 475,\n",
       "             '最も': 476,\n",
       "             '台': 477,\n",
       "             'せた': 478,\n",
       "             '制作': 479,\n",
       "             '人口': 480,\n",
       "             '２７': 481,\n",
       "             '施設': 482,\n",
       "             '正': 483,\n",
       "             'ただし': 484,\n",
       "             '公': 485,\n",
       "             '終了': 486,\n",
       "             '製造': 487,\n",
       "             '対する': 488,\n",
       "             '##リ': 489,\n",
       "             '船': 490,\n",
       "             '部分': 491,\n",
       "             '関する': 492,\n",
       "             'Ｐ': 493,\n",
       "             '高校': 494,\n",
       "             '##ト': 495,\n",
       "             '総': 496,\n",
       "             '石': 497,\n",
       "             '組織': 498,\n",
       "             '公開': 499,\n",
       "             'ほど': 500,\n",
       "             '教授': 501,\n",
       "             '収録': 502,\n",
       "             'Ｆ': 503,\n",
       "             '無': 504,\n",
       "             '株式': 505,\n",
       "             '生産': 506,\n",
       "             'ロシア': 507,\n",
       "             '２９': 508,\n",
       "             '車両': 509,\n",
       "             '必要': 510,\n",
       "             '最': 511,\n",
       "             '使わ': 512,\n",
       "             '海軍': 513,\n",
       "             '全国': 514,\n",
       "             '列車': 515,\n",
       "             '帝国': 516,\n",
       "             '##コ': 517,\n",
       "             '野球': 518,\n",
       "             '用い': 519,\n",
       "             '書': 520,\n",
       "             '##川': 521,\n",
       "             '大きな': 522,\n",
       "             '全て': 523,\n",
       "             'Ｔ': 524,\n",
       "             'ラ': 525,\n",
       "             '初めて': 526,\n",
       "             'シングル': 527,\n",
       "             'モデル': 528,\n",
       "             '枚': 529,\n",
       "             'Ｋ': 530,\n",
       "             'Ｒ': 531,\n",
       "             '##フ': 532,\n",
       "             '##ダ': 533,\n",
       "             '面': 534,\n",
       "             '四': 535,\n",
       "             'Ｇ': 536,\n",
       "             '高い': 537,\n",
       "             '２００４': 538,\n",
       "             '議員': 539,\n",
       "             '部隊': 540,\n",
       "             '率': 541,\n",
       "             '主に': 542,\n",
       "             '受賞': 543,\n",
       "             'メンバー': 544,\n",
       "             '名称': 545,\n",
       "             '企業': 546,\n",
       "             '〜': 547,\n",
       "             '目的': 548,\n",
       "             '最終': 549,\n",
       "             '##ド': 550,\n",
       "             '道路': 551,\n",
       "             '##ティ': 552,\n",
       "             '勝利': 553,\n",
       "             '##ン': 554,\n",
       "             '##カ': 555,\n",
       "             'そこ': 556,\n",
       "             '機能': 557,\n",
       "             'センター': 558,\n",
       "             '京都': 559,\n",
       "             '運動': 560,\n",
       "             '受けた': 561,\n",
       "             '末': 562,\n",
       "             '員': 563,\n",
       "             '活躍': 564,\n",
       "             '店': 565,\n",
       "             '１００': 566,\n",
       "             '管理': 567,\n",
       "             '党': 568,\n",
       "             '独立': 569,\n",
       "             '実施': 570,\n",
       "             '移籍': 571,\n",
       "             '取り': 572,\n",
       "             '海': 573,\n",
       "             '##い': 574,\n",
       "             '人物': 575,\n",
       "             '時期': 576,\n",
       "             'ほとんど': 577,\n",
       "             '##ム': 578,\n",
       "             'Ｊ': 579,\n",
       "             '生活': 580,\n",
       "             '務めた': 581,\n",
       "             '戦い': 582,\n",
       "             'Ｌ': 583,\n",
       "             '経て': 584,\n",
       "             '入り': 585,\n",
       "             '通常': 586,\n",
       "             '製作': 587,\n",
       "             'Ｈ': 588,\n",
       "             '##ル': 589,\n",
       "             'サッカー': 590,\n",
       "             '構造': 591,\n",
       "             '馬': 592,\n",
       "             'クラブ': 593,\n",
       "             '年度': 594,\n",
       "             '対応': 595,\n",
       "             '教会': 596,\n",
       "             'カ': 597,\n",
       "             '##ロ': 598,\n",
       "             '朝': 599,\n",
       "             '成功': 600,\n",
       "             'デ': 601,\n",
       "             'エンジン': 602,\n",
       "             '量': 603,\n",
       "             'ところ': 604,\n",
       "             'イタリア': 605,\n",
       "             '発見': 606,\n",
       "             '例': 607,\n",
       "             '基本': 608,\n",
       "             '含む': 609,\n",
       "             '江戸': 610,\n",
       "             '重': 611,\n",
       "             '戦闘': 612,\n",
       "             '受けて': 613,\n",
       "             '##レ': 614,\n",
       "             '府': 615,\n",
       "             '以前': 616,\n",
       "             '##って': 617,\n",
       "             '##シ': 618,\n",
       "             'Ｎ': 619,\n",
       "             '##ズ': 620,\n",
       "             '##リー': 621,\n",
       "             '##ア': 622,\n",
       "             '設定': 623,\n",
       "             '３１': 624,\n",
       "             '実際': 625,\n",
       "             'キャラクター': 626,\n",
       "             '搭載': 627,\n",
       "             '名前': 628,\n",
       "             '##ニ': 629,\n",
       "             '非': 630,\n",
       "             'ここ': 631,\n",
       "             '規模': 632,\n",
       "             '対': 633,\n",
       "             '協会': 634,\n",
       "             '##ら': 635,\n",
       "             '２００３': 636,\n",
       "             'アニメ': 637,\n",
       "             'ホーム': 638,\n",
       "             '内容': 639,\n",
       "             '設計': 640,\n",
       "             'アル': 641,\n",
       "             '##った': 642,\n",
       "             '##イ': 643,\n",
       "             '##野': 644,\n",
       "             'Ｖ': 645,\n",
       "             '漫画': 646,\n",
       "             '編成': 647,\n",
       "             '決勝': 648,\n",
       "             '##ツ': 649,\n",
       "             '指定': 650,\n",
       "             'リ': 651,\n",
       "             '人間': 652,\n",
       "             '室': 653,\n",
       "             '作曲': 654,\n",
       "             'エ': 655,\n",
       "             '評価': 656,\n",
       "             '##サ': 657,\n",
       "             '特徴': 658,\n",
       "             'ドラマ': 659,\n",
       "             '##ジ': 660,\n",
       "             '女子': 661,\n",
       "             '指揮': 662,\n",
       "             '科学': 663,\n",
       "             '交通': 664,\n",
       "             'マ': 665,\n",
       "             '異なる': 666,\n",
       "             '再び': 667,\n",
       "             '##る': 668,\n",
       "             '出版': 669,\n",
       "             '最初の': 670,\n",
       "             '運行': 671,\n",
       "             '##き': 672,\n",
       "             '専門': 673,\n",
       "             '副': 674,\n",
       "             'ヨーロッパ': 675,\n",
       "             '##マ': 676,\n",
       "             '##チ': 677,\n",
       "             '先': 678,\n",
       "             '指導': 679,\n",
       "             '連合': 680,\n",
       "             '運営': 681,\n",
       "             '制': 682,\n",
       "             'み': 683,\n",
       "             '五': 684,\n",
       "             '予定': 685,\n",
       "             '国家': 686,\n",
       "             'イ': 687,\n",
       "             '光': 688,\n",
       "             '期間': 689,\n",
       "             '大きく': 690,\n",
       "             '表記': 691,\n",
       "             '主': 692,\n",
       "             '対し': 693,\n",
       "             '彼女': 694,\n",
       "             '品': 695,\n",
       "             '結婚': 696,\n",
       "             '向け': 697,\n",
       "             '翌': 698,\n",
       "             '’': 699,\n",
       "             'バンド': 700,\n",
       "             'リリース': 701,\n",
       "             '##Ｓ': 702,\n",
       "             '対象': 703,\n",
       "             '団': 704,\n",
       "             '連続': 705,\n",
       "             '５０': 706,\n",
       "             '経営': 707,\n",
       "             '歌': 708,\n",
       "             '廃止': 709,\n",
       "             '色': 710,\n",
       "             '多数': 711,\n",
       "             '##く': 712,\n",
       "             'タイトル': 713,\n",
       "             '団体': 714,\n",
       "             '##ガ': 715,\n",
       "             '##キ': 716,\n",
       "             '時点': 717,\n",
       "             '２００２': 718,\n",
       "             '当': 719,\n",
       "             'すべて': 720,\n",
       "             'しまう': 721,\n",
       "             '##ｔ': 722,\n",
       "             'あるいは': 723,\n",
       "             '条': 724,\n",
       "             '##ブ': 725,\n",
       "             '様々な': 726,\n",
       "             '程度': 727,\n",
       "             '小説': 728,\n",
       "             '藩': 729,\n",
       "             '記念': 730,\n",
       "             '製': 731,\n",
       "             '院': 732,\n",
       "             '英語': 733,\n",
       "             '参照': 734,\n",
       "             '理由': 735,\n",
       "             '大戦': 736,\n",
       "             '持ち': 737,\n",
       "             '##ｉｎ': 738,\n",
       "             '事務': 739,\n",
       "             '提供': 740,\n",
       "             'ド': 741,\n",
       "             '導入': 742,\n",
       "             '自動車': 743,\n",
       "             '群': 744,\n",
       "             'Ｅ': 745,\n",
       "             '１９９０': 746,\n",
       "             'ローマ': 747,\n",
       "             '２０１８': 748,\n",
       "             'オ': 749,\n",
       "             '##テ': 750,\n",
       "             '死去': 751,\n",
       "             '４０': 752,\n",
       "             '人気': 753,\n",
       "             '##ー': 754,\n",
       "             'たい': 755,\n",
       "             '２００１': 756,\n",
       "             '達': 757,\n",
       "             '##ディ': 758,\n",
       "             '労働': 759,\n",
       "             '##島': 760,\n",
       "             '生まれ': 761,\n",
       "             'よう': 762,\n",
       "             '展開': 763,\n",
       "             '##バ': 764,\n",
       "             '周辺': 765,\n",
       "             'ザ': 766,\n",
       "             '全体': 767,\n",
       "             'デザイン': 768,\n",
       "             '運転': 769,\n",
       "             '基': 770,\n",
       "             '方法': 771,\n",
       "             '領': 772,\n",
       "             'サービス': 773,\n",
       "             '今': 774,\n",
       "             '##ｅｎ': 775,\n",
       "             'その他': 776,\n",
       "             'サン': 777,\n",
       "             '形式': 778,\n",
       "             'コ': 779,\n",
       "             '真': 780,\n",
       "             '公園': 781,\n",
       "             '##ネ': 782,\n",
       "             '橋': 783,\n",
       "             '以来': 784,\n",
       "             '##ｏｎ': 785,\n",
       "             '白': 786,\n",
       "             'ラジオ': 787,\n",
       "             '小学校': 788,\n",
       "             'サ': 789,\n",
       "             'しか': 790,\n",
       "             '説': 791,\n",
       "             '兵': 792,\n",
       "             '結成': 793,\n",
       "             '##一': 794,\n",
       "             '議会': 795,\n",
       "             '総合': 796,\n",
       "             '同時に': 797,\n",
       "             '大統領': 798,\n",
       "             '##ａｌ': 799,\n",
       "             'のち': 800,\n",
       "             '##人': 801,\n",
       "             'マン': 802,\n",
       "             '##Ｌ': 803,\n",
       "             '警察': 804,\n",
       "             '舞台': 805,\n",
       "             '環境': 806,\n",
       "             '言わ': 807,\n",
       "             '宇宙': 808,\n",
       "             '支援': 809,\n",
       "             '引退': 810,\n",
       "             '##ｄ': 811,\n",
       "             '娘': 812,\n",
       "             '##グ': 813,\n",
       "             '関連': 814,\n",
       "             '中学校': 815,\n",
       "             '最高': 816,\n",
       "             '国民': 817,\n",
       "             '由来': 818,\n",
       "             '会議': 819,\n",
       "             '初期': 820,\n",
       "             'スーパー': 821,\n",
       "             '持って': 822,\n",
       "             'ほぼ': 823,\n",
       "             '競技': 824,\n",
       "             '自治': 825,\n",
       "             '飛行': 826,\n",
       "             '自ら': 827,\n",
       "             '連邦': 828,\n",
       "             'バ': 829,\n",
       "             'スポーツ': 830,\n",
       "             '##ｎ': 831,\n",
       "             '完成': 832,\n",
       "             'いずれ': 833,\n",
       "             '含ま': 834,\n",
       "             '夏': 835,\n",
       "             '北海道': 836,\n",
       "             '裁判': 837,\n",
       "             'ドル': 838,\n",
       "             '陸軍': 839,\n",
       "             '##Ｔ': 840,\n",
       "             '組': 841,\n",
       "             '街': 842,\n",
       "             '##Ｃ': 843,\n",
       "             'ひと': 844,\n",
       "             '行い': 845,\n",
       "             '＿': 846,\n",
       "             'カード': 847,\n",
       "             '##ウ': 848,\n",
       "             'ｃｍ': 849,\n",
       "             'ル': 850,\n",
       "             '変化': 851,\n",
       "             '１９８０': 852,\n",
       "             '巻': 853,\n",
       "             '共同': 854,\n",
       "             '特別': 855,\n",
       "             '##か': 856,\n",
       "             '寺': 857,\n",
       "             '支配': 858,\n",
       "             '花': 859,\n",
       "             '士': 860,\n",
       "             'スペイン': 861,\n",
       "             '主張': 862,\n",
       "             '大正': 863,\n",
       "             'オリンピック': 864,\n",
       "             '類': 865,\n",
       "             '天': 866,\n",
       "             '学者': 867,\n",
       "             '##さ': 868,\n",
       "             'アン': 869,\n",
       "             'Ｗ': 870,\n",
       "             '##み': 871,\n",
       "             '対戦': 872,\n",
       "             'とって': 873,\n",
       "             '##ａｎ': 874,\n",
       "             '妻': 875,\n",
       "             '丁目': 876,\n",
       "             'アジア': 877,\n",
       "             'パ': 878,\n",
       "             '文': 879,\n",
       "             '校': 880,\n",
       "             '個': 881,\n",
       "             '##ｅｓ': 882,\n",
       "             '建築': 883,\n",
       "             '##Ｂ': 884,\n",
       "             '頭': 885,\n",
       "             'よく': 886,\n",
       "             '整備': 887,\n",
       "             '撮影': 888,\n",
       "             '##ミ': 889,\n",
       "             '形成': 890,\n",
       "             '作戦': 891,\n",
       "             '演奏': 892,\n",
       "             '共和': 893,\n",
       "             '##プ': 894,\n",
       "             'レ': 895,\n",
       "             '英': 896,\n",
       "             '神社': 897,\n",
       "             '移動': 898,\n",
       "             '右': 899,\n",
       "             '輸送': 900,\n",
       "             '与え': 901,\n",
       "             '##Ｏ': 902,\n",
       "             '##Ｅ': 903,\n",
       "             '制度': 904,\n",
       "             '１９９９': 905,\n",
       "             'ニューヨーク': 906,\n",
       "             '法人': 907,\n",
       "             '能力': 908,\n",
       "             '発展': 909,\n",
       "             '左': 910,\n",
       "             '営業': 911,\n",
       "             'レース': 912,\n",
       "             '非常に': 913,\n",
       "             'ハ': 914,\n",
       "             '文字': 915,\n",
       "             '登録': 916,\n",
       "             '##マン': 917,\n",
       "             '子供': 918,\n",
       "             '状況': 919,\n",
       "             '##木': 920,\n",
       "             '伴い': 921,\n",
       "             '準': 922,\n",
       "             'ス': 923,\n",
       "             '風': 924,\n",
       "             '表現': 925,\n",
       "             '母': 926,\n",
       "             '##Ｄ': 927,\n",
       "             'ファン': 928,\n",
       "             '産業': 929,\n",
       "             '韓国': 930,\n",
       "             'まま': 931,\n",
       "             '表示': 932,\n",
       "             'イン': 933,\n",
       "             '圏': 934,\n",
       "             '企画': 935,\n",
       "             '以後': 936,\n",
       "             '試験': 937,\n",
       "             '##ゴ': 938,\n",
       "             '公式': 939,\n",
       "             '着': 940,\n",
       "             '得': 941,\n",
       "             '##Ｆ': 942,\n",
       "             '男': 943,\n",
       "             'データ': 944,\n",
       "             '発行': 945,\n",
       "             '行政': 946,\n",
       "             '##トン': 947,\n",
       "             '誌': 948,\n",
       "             '区間': 949,\n",
       "             '##す': 950,\n",
       "             '##ラー': 951,\n",
       "             '##デ': 952,\n",
       "             '##Ｇ': 953,\n",
       "             'す': 954,\n",
       "             '御': 955,\n",
       "             '春': 956,\n",
       "             '論': 957,\n",
       "             '姿': 958,\n",
       "             '合併': 959,\n",
       "             '##Ｋ': 960,\n",
       "             '距離': 961,\n",
       "             '複数': 962,\n",
       "             '心': 963,\n",
       "             '##である': 964,\n",
       "             '##ｕ': 965,\n",
       "             '認め': 966,\n",
       "             '新しい': 967,\n",
       "             '方面': 968,\n",
       "             '##ｍ': 969,\n",
       "             '##スト': 970,\n",
       "             '選出': 971,\n",
       "             '分類': 972,\n",
       "             '種類': 973,\n",
       "             '広島': 974,\n",
       "             '口': 975,\n",
       "             '１９７０': 976,\n",
       "             'ｍｍ': 977,\n",
       "             '##オ': 978,\n",
       "             '階': 979,\n",
       "             '新聞': 980,\n",
       "             '##リン': 981,\n",
       "             '赤': 982,\n",
       "             'シ': 983,\n",
       "             '番号': 984,\n",
       "             '##ｈ': 985,\n",
       "             '成立': 986,\n",
       "             '工業': 987,\n",
       "             '##ｉｎｇ': 988,\n",
       "             '##れ': 989,\n",
       "             'なければ': 990,\n",
       "             '工場': 991,\n",
       "             '運用': 992,\n",
       "             '言語': 993,\n",
       "             '自由': 994,\n",
       "             '##ｌ': 995,\n",
       "             '##ｃ': 996,\n",
       "             '南部': 997,\n",
       "             '装置': 998,\n",
       "             '##ｋ': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "batch_size = 32  # BERTでは16、32あたりを使用する\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_val_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認 検証データのデータセットで確認\n",
    "batch = next(iter(train_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの1文目を確認してみる\n",
    "text_minibatch_1 = (batch.Text[0][1]).numpy()\n",
    "\n",
    "# IDを単語に戻す\n",
    "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感情分析用のBERTモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight→embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight→embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight→embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight→embeddings.LayerNorm.gamma\n",
      "bert.embeddings.LayerNorm.bias→embeddings.LayerNorm.beta\n",
      "bert.encoder.layer.0.attention.self.query.weight→encoder.layer.0.attention.selfattn.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias→encoder.layer.0.attention.selfattn.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight→encoder.layer.0.attention.selfattn.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias→encoder.layer.0.attention.selfattn.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight→encoder.layer.0.attention.selfattn.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias→encoder.layer.0.attention.selfattn.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight→encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias→encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight→encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias→encoder.layer.0.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.0.intermediate.dense.weight→encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias→encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight→encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias→encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight→encoder.layer.0.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.output.LayerNorm.bias→encoder.layer.0.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.attention.self.query.weight→encoder.layer.1.attention.selfattn.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias→encoder.layer.1.attention.selfattn.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight→encoder.layer.1.attention.selfattn.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias→encoder.layer.1.attention.selfattn.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight→encoder.layer.1.attention.selfattn.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias→encoder.layer.1.attention.selfattn.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight→encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias→encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight→encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias→encoder.layer.1.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.intermediate.dense.weight→encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias→encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight→encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias→encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight→encoder.layer.1.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.output.LayerNorm.bias→encoder.layer.1.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.attention.self.query.weight→encoder.layer.2.attention.selfattn.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias→encoder.layer.2.attention.selfattn.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight→encoder.layer.2.attention.selfattn.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias→encoder.layer.2.attention.selfattn.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight→encoder.layer.2.attention.selfattn.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias→encoder.layer.2.attention.selfattn.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight→encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias→encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight→encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias→encoder.layer.2.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.intermediate.dense.weight→encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias→encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight→encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias→encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight→encoder.layer.2.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.output.LayerNorm.bias→encoder.layer.2.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.attention.self.query.weight→encoder.layer.3.attention.selfattn.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias→encoder.layer.3.attention.selfattn.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight→encoder.layer.3.attention.selfattn.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias→encoder.layer.3.attention.selfattn.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight→encoder.layer.3.attention.selfattn.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias→encoder.layer.3.attention.selfattn.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight→encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias→encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight→encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias→encoder.layer.3.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.intermediate.dense.weight→encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias→encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight→encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias→encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight→encoder.layer.3.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.output.LayerNorm.bias→encoder.layer.3.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.attention.self.query.weight→encoder.layer.4.attention.selfattn.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias→encoder.layer.4.attention.selfattn.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight→encoder.layer.4.attention.selfattn.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias→encoder.layer.4.attention.selfattn.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight→encoder.layer.4.attention.selfattn.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias→encoder.layer.4.attention.selfattn.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight→encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias→encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight→encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias→encoder.layer.4.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.intermediate.dense.weight→encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias→encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight→encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias→encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight→encoder.layer.4.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.output.LayerNorm.bias→encoder.layer.4.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.attention.self.query.weight→encoder.layer.5.attention.selfattn.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias→encoder.layer.5.attention.selfattn.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight→encoder.layer.5.attention.selfattn.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias→encoder.layer.5.attention.selfattn.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight→encoder.layer.5.attention.selfattn.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias→encoder.layer.5.attention.selfattn.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight→encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias→encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight→encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias→encoder.layer.5.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.intermediate.dense.weight→encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias→encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight→encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias→encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight→encoder.layer.5.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.output.LayerNorm.bias→encoder.layer.5.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.attention.self.query.weight→encoder.layer.6.attention.selfattn.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias→encoder.layer.6.attention.selfattn.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight→encoder.layer.6.attention.selfattn.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias→encoder.layer.6.attention.selfattn.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight→encoder.layer.6.attention.selfattn.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias→encoder.layer.6.attention.selfattn.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight→encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias→encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight→encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias→encoder.layer.6.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.intermediate.dense.weight→encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias→encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight→encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias→encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight→encoder.layer.6.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.output.LayerNorm.bias→encoder.layer.6.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.attention.self.query.weight→encoder.layer.7.attention.selfattn.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias→encoder.layer.7.attention.selfattn.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight→encoder.layer.7.attention.selfattn.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias→encoder.layer.7.attention.selfattn.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight→encoder.layer.7.attention.selfattn.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias→encoder.layer.7.attention.selfattn.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight→encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias→encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight→encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias→encoder.layer.7.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.intermediate.dense.weight→encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias→encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight→encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias→encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight→encoder.layer.7.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.output.LayerNorm.bias→encoder.layer.7.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.attention.self.query.weight→encoder.layer.8.attention.selfattn.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias→encoder.layer.8.attention.selfattn.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight→encoder.layer.8.attention.selfattn.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias→encoder.layer.8.attention.selfattn.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight→encoder.layer.8.attention.selfattn.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias→encoder.layer.8.attention.selfattn.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight→encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias→encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight→encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias→encoder.layer.8.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.intermediate.dense.weight→encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias→encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight→encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias→encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight→encoder.layer.8.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.output.LayerNorm.bias→encoder.layer.8.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.attention.self.query.weight→encoder.layer.9.attention.selfattn.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias→encoder.layer.9.attention.selfattn.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight→encoder.layer.9.attention.selfattn.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias→encoder.layer.9.attention.selfattn.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight→encoder.layer.9.attention.selfattn.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias→encoder.layer.9.attention.selfattn.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight→encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias→encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight→encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias→encoder.layer.9.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.intermediate.dense.weight→encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias→encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight→encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias→encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight→encoder.layer.9.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.output.LayerNorm.bias→encoder.layer.9.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.attention.self.query.weight→encoder.layer.10.attention.selfattn.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias→encoder.layer.10.attention.selfattn.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight→encoder.layer.10.attention.selfattn.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias→encoder.layer.10.attention.selfattn.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight→encoder.layer.10.attention.selfattn.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias→encoder.layer.10.attention.selfattn.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight→encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias→encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight→encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias→encoder.layer.10.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.intermediate.dense.weight→encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias→encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight→encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias→encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight→encoder.layer.10.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.output.LayerNorm.bias→encoder.layer.10.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.attention.self.query.weight→encoder.layer.11.attention.selfattn.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias→encoder.layer.11.attention.selfattn.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight→encoder.layer.11.attention.selfattn.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias→encoder.layer.11.attention.selfattn.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight→encoder.layer.11.attention.selfattn.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias→encoder.layer.11.attention.selfattn.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight→encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias→encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight→encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias→encoder.layer.11.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.intermediate.dense.weight→encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias→encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight→encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias→encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight→encoder.layer.11.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.output.LayerNorm.bias→encoder.layer.11.output.LayerNorm.beta\n",
      "bert.pooler.dense.weight→pooler.dense.weight\n",
      "bert.pooler.dense.bias→pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "from utils.bert import get_config, BertModel, set_learned_params\n",
    "\n",
    "# モデル設定のJOSNファイルをオブジェクト変数として読み込みます\n",
    "config = get_config(file_path=\"./weights/bert_config.json\")\n",
    "\n",
    "# BERTモデルを作成します\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "# BERTモデルに学習済みパラメータセットします\n",
    "net_bert = set_learned_params(\n",
    "    net_bert, weights_path=\"./weights/pytorch_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForIMDb(nn.Module):\n",
    "    '''BERTモデルにIMDbのポジ・ネガを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForIMDb, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "\n",
    "        # headにポジネガ予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元、出力はポジ・ネガの2つ\n",
    "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # 入力文章の1単語目[CLS]の特徴量を使用して、ポジ・ネガを分類します\n",
    "        vec_0 = encoded_layers[:, 0, :]\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
    "        out = self.cls(vec_0)\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BertForIMDb(net_bert)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのファインチューニングに向けた設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 最後のBertLayerモジュールを勾配計算ありに変更\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・検証を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BertForIMDbに入力\n",
    "                    outputs = net(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                  output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            t_epoch_start = time.time()\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 15\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したネットワークパラメータを保存します\n",
    "save_path = './weights/bert_fine_tuning_chABSA_4epoch.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)\n",
    "#the_model = TheModelClass(*args, **kwargs)\n",
    "#the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForIMDb(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32006, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (selfattn): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデル構築\n",
    "net_trained = BertForIMDb(net_bert)\n",
    "save_path = './weights/bert_fine_tuning_1_22_08671_epoch_chABSA.pth'\n",
    "# 学習したネットワークパラメータをロード\n",
    "net_trained.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "net_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの正解率を求める\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(val_dl):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForIMDbに入力\n",
    "        outputs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                              output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(val_dl.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(val_dl.dataset), epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentionの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_sizeを64にしたテストデータでDataLoaderを作成\n",
    "batch_size = 32\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForIMDbで処理\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ミニバッチの用意\n",
    "batch = next(iter(test_dl))\n",
    "\n",
    "# GPUが使えるならGPUにデータを送る\n",
    "inputs = batch.Text[0].to(device)  # 文章\n",
    "labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "outputs, attention_probs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                       output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "_, preds = torch.max(outputs, 1)  # ラベルを予測\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLを作成する関数を実装\n",
    "\n",
    "\n",
    "def highlight(word, attn):\n",
    "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
    "\n",
    "    html_color = '#%02X%02X%02X' % (\n",
    "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "\n",
    "def mk_html(index, batch, preds, normlized_weights, TEXT):\n",
    "    \"HTMLデータを作成する\"\n",
    "\n",
    "    # indexの結果を抽出\n",
    "    sentence = batch.Text[0][index]  # 文章\n",
    "    label = batch.Label[index]  # ラベル\n",
    "    pred = preds[index]  # 予測\n",
    "\n",
    "    # ラベルと予測結果を文字に置き換え\n",
    "    if label == 0:\n",
    "        label_str = \"Negative\"\n",
    "    else:\n",
    "        label_str = \"Positive\"\n",
    "\n",
    "    if pred == 0:\n",
    "        pred_str = \"Negative\"\n",
    "    else:\n",
    "        pred_str = \"Positive\"\n",
    "\n",
    "    # 表示用のHTMLを作成する\n",
    "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
    "\n",
    "    # Self-Attentionの重みを可視化。Multi-Headが12個なので、12種類のアテンションが存在\n",
    "    for i in range(12):\n",
    "\n",
    "        # indexのAttentionを抽出と規格化\n",
    "        # 0単語目[CLS]の、i番目のMulti-Head Attentionを取り出す\n",
    "        # indexはミニバッチの何個目のデータかをしめす\n",
    "        attens = normlized_weights[index, i, 0, :]\n",
    "        attens /= attens.max()\n",
    "\n",
    "        html += '[BERTのAttentionを可視化_' + str(i+1) + ']<br>'\n",
    "        for word, attn in zip(sentence, attens):\n",
    "\n",
    "            # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "            if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "                break\n",
    "\n",
    "            # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "            html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "                [word.numpy().tolist()])[0], attn)\n",
    "        html += \"<br><br>\"\n",
    "\n",
    "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
    "    all_attens = attens*0  # all_attensという変数を作成する\n",
    "    for i in range(12):\n",
    "        attens += normlized_weights[index, i, 0, :]\n",
    "    attens /= attens.max()\n",
    "\n",
    "    html += '[BERTのAttentionを可視化_ALL]<br>'\n",
    "    for word, attn in zip(sentence, attens):\n",
    "\n",
    "        # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "        if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "            break\n",
    "\n",
    "        # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "        html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "            [word.numpy().tolist()])[0], attn)\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "index = 1  # 出力させたいデータ\n",
    "html_output = mk_html(index, batch, preds, attention_probs, TEXT)  # HTML作成\n",
    "HTML(html_output)  # HTML形式で出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 61  # 出力させたいデータ\n",
    "html_output = mk_html(index, batch, preds, attention_probs, TEXT)  # HTML作成\n",
    "HTML(html_output)  # HTML形式で出力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論用の1文章をインプットしてラベルとAttentionを可視化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "\n",
    "tokenizer_bert = BertTokenizer(\n",
    "    vocab_file=\"./vocab/vocab.txt\", do_lower_case=False)\n",
    "\n",
    "def replace_unk(text):\n",
    "    tokens = []\n",
    "\n",
    "# 前処理と単語分割をまとめた関数を定義\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)  # tokenizer_bert\n",
    "    return ret\n",
    "\n",
    "def create_tensor(text, max_length):\n",
    "    #入力文章をTorch Teonsor型にのINDEXデータに変換\n",
    "    token_ids = torch.ones((max_length)).to(torch.int64)\n",
    "    ids_list = list(map(lambda x: TEXT.vocab.stoi[x] , text))\n",
    "    print(ids_list)\n",
    "    for i, index in enumerate(ids_list):\n",
    "        token_ids[i] = index\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "# HTMLを作成する関数を実装\n",
    "\n",
    "\n",
    "def highlight(word, attn):\n",
    "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
    "\n",
    "    html_color = '#%02X%02X%02X' % (\n",
    "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "\n",
    "def mk_html(input, preds, normlized_weights, TEXT):\n",
    "    \"HTMLデータを作成する\"\n",
    "\n",
    "    # indexの結果を抽出\n",
    "    index = 0\n",
    "    sentence = input.squeeze_(0) # 文章  #  torch.Size([1, 256])  > torch.Size([256]) \n",
    "    pred = preds[0]  # 予測\n",
    "\n",
    "\n",
    "    # 予測結果を文字に置き換え\n",
    "    if pred == 0:\n",
    "        pred_str = \"Negative\"\n",
    "    else:\n",
    "        pred_str = \"Positive\"\n",
    "\n",
    "    # 表示用のHTMLを作成する\n",
    "\n",
    "    html = '推論ラベル：{}<br><br>'.format(pred_str)\n",
    "    # Self-Attentionの重みを可視化。Multi-Headが12個なので、12種類のアテンションが存在\n",
    "\n",
    "    for i in range(12):\n",
    "\n",
    "        # indexのAttentionを抽出と規格化\n",
    "        # 0単語目[CLS]の、i番目のMulti-Head Attentionを取り出す\n",
    "        # indexはミニバッチの何個目のデータかをしめす\n",
    "        attens = normlized_weights[index, i, 0, :]\n",
    "        attens /= attens.max()\n",
    "\n",
    "        #html += '[BERTのAttentionを可視化_' + str(i+1) + ']<br>'\n",
    "        for word, attn in zip(sentence, attens):\n",
    "\n",
    "            # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "            if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "                break\n",
    "\n",
    "            # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "            #html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "            #    [word.numpy().tolist()])[0], attn)\n",
    "        #html += \"<br><br>\"\n",
    "\n",
    "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
    "    all_attens = attens*0  # all_attensという変数を作成する\n",
    "    for i in range(12):\n",
    "        attens += normlized_weights[index, i, 0, :]\n",
    "    attens /= attens.max()\n",
    "\n",
    "    html += '[BERTのAttentionを可視化_ALL]<br>'\n",
    "    for word, attn in zip(sentence, attens):\n",
    "\n",
    "        # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "        if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "            break\n",
    "\n",
    "        # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "        html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "            [word.numpy().tolist()])[0], attn)\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以上', 'の', '結果', '、', '当', '連結', '会計', '年度', 'に', 'おける', '売上高', '[UNK]', '，', '[UNK]', '円', '（', '前年', '同期', '比', '[UNK]', '円', '減', '、', '[UNK]', '．', '[UNK]', '％', '減', '）', '、', '営業', '損失', '[UNK]', '円', '（', '前年', '同期', '比', '[UNK]', '円', '減', '、', '前年', '同期', '営業', '利益', '[UNK]', '円', '）', '、', '[UNK]', '損失', '[UNK]', '円', '（', '前年', '同期', '比', '[UNK]', '円', '減', '、', '前年', '同期', '[UNK]', '利益', '[UNK]', '円', '）', '、', '親会社', '株主', 'に', '帰属', 'する', '[UNK]', '純', '損失', '[UNK]', '円', '（', '前年', '同期', '比', '[UNK]', '円', '減', '、', '前年', '同期', '親会社', '株主', 'に', '帰属', 'する', '[UNK]', '純', '利益', '[UNK]', '円', '）', 'と', 'なり', 'ました']\n",
      "[2, 269, 5, 337, 6, 719, 3700, 5481, 594, 8, 217, 16720, 1, 176, 1, 387, 16, 2307, 3704, 2460, 1, 387, 4265, 6, 1, 264, 1, 257, 4265, 17, 6, 911, 7429, 1, 387, 16, 2307, 3704, 2460, 1, 387, 4265, 6, 2307, 3704, 911, 3718, 1, 387, 17, 6, 1, 7429, 1, 387, 16, 2307, 3704, 2460, 1, 387, 4265, 6, 2307, 3704, 1, 3718, 1, 387, 17, 6, 11100, 6970, 8, 13937, 22, 1, 3962, 7429, 1, 387, 16, 2307, 3704, 2460, 1, 387, 4265, 6, 2307, 3704, 11100, 6970, 8, 13937, 22, 1, 3962, 3718, 1, 387, 17, 12, 105, 4561, 3]\n",
      "input_shape= torch.Size([1, 256])\n",
      "tensor([[    2,   269,     5,   337,     6,   719,  3700,  5481,   594,     8,\n",
      "           217, 16720,     1,   176,     1,   387,    16,  2307,  3704,  2460,\n",
      "             1,   387,  4265,     6,     1,   264,     1,   257,  4265,    17,\n",
      "             6,   911,  7429,     1,   387,    16,  2307,  3704,  2460,     1,\n",
      "           387,  4265,     6,  2307,  3704,   911,  3718,     1,   387,    17,\n",
      "             6,     1,  7429,     1,   387,    16,  2307,  3704,  2460,     1,\n",
      "           387,  4265,     6,  2307,  3704,     1,  3718,     1,   387,    17,\n",
      "             6, 11100,  6970,     8, 13937,    22,     1,  3962,  7429,     1,\n",
      "           387,    16,  2307,  3704,  2460,     1,   387,  4265,     6,  2307,\n",
      "          3704, 11100,  6970,     8, 13937,    22,     1,  3962,  3718,     1,\n",
      "           387,    17,    12,   105,  4561,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "         0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "#from utils.dataloader import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)\n",
    "\n",
    "#インプットデータ\n",
    "input_text = \"以上の結果、当連結会計年度における売上高1,785百万円(前年同期比357百万円減、16.7％減)、営業損失117百万円(前年同期比174百万円減、前年同期　営業利益57百万円)、経常損失112百万円(前年同期比183百万円減、前年同期　経常利益71百万円)、親会社株主に帰属する当期純損失58百万円(前年同期比116百万円減、前年同期　親会社株主に帰属する当期純利益57百万円)となりました\"\n",
    "#textの先頭と末尾に<cls>、<eos>を追加する。\n",
    "text = tokenizer_with_preprocessing(input_text)\n",
    "text.insert(0, '[CLS]')\n",
    "text.append('[SEP]')\n",
    "#   '[CLS]', 2, '[SEP]', 3\n",
    "\n",
    "            \n",
    "text = create_tensor(text, 256)\n",
    "text = text.unsqueeze_(0)   #  torch.Size([256])  > torch.Size([1, 256])\n",
    "\n",
    "# GPUが使えるならGPUにデータを送る\n",
    "input = text.to(device)\n",
    "print(\"input_shape=\",input.shape)\n",
    "# mask作成\n",
    "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "input_mask = (input != input_pad)\n",
    "print(input)\n",
    "print(input_mask)\n",
    "\n",
    "\n",
    "outputs, attention_probs = net_trained(input, token_type_ids=None, attention_mask=None,\n",
    "                                       output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "\n",
    "html_output = mk_html(input, preds, attention_probs, TEXT)  # HTML作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs, attention_probs = net_trained(input, token_type_ids=None, attention_mask=None,\n",
    "                                       output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "\n",
    "html_output = mk_html(input, preds, attention_probs, TEXT)  # HTML作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力文章\n",
      "以上の結果、当連結会計年度における売上高1,785百万円(前年同期比357百万円減、16.7％減)、営業損失117百万円(前年同期比174百万円減、前年同期　営業利益57百万円)、経常損失112百万円(前年同期比183百万円減、前年同期　経常利益71百万円)、親会社株主に帰属する当期純損失58百万円(前年同期比116百万円減、前年同期　親会社株主に帰属する当期純利益57百万円)となりました\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "推論ラベル：Negative<br><br>[BERTのAttentionを可視化_ALL]<br><span style=\"background-color: #FFFDFD\"> [CLS]</span><span style=\"background-color: #FF8E8E\"> 以上</span><span style=\"background-color: #FFF1F1\"> の</span><span style=\"background-color: #FFD8D8\"> 結果</span><span style=\"background-color: #FFF9F9\"> 、</span><span style=\"background-color: #FFD9D9\"> 当</span><span style=\"background-color: #FFE5E5\"> 連結</span><span style=\"background-color: #FFAAAA\"> 会計</span><span style=\"background-color: #FFEFEF\"> 年度</span><span style=\"background-color: #FFFDFD\"> に</span><span style=\"background-color: #FFECEC\"> おける</span><span style=\"background-color: #FFD1D1\"> 売上高</span><span style=\"background-color: #FFE5E5\"> [UNK]</span><span style=\"background-color: #FFF7F7\"> ，</span><span style=\"background-color: #FFF4F4\"> [UNK]</span><span style=\"background-color: #FFD7D7\"> 円</span><span style=\"background-color: #FFFEFE\"> （</span><span style=\"background-color: #FFFBFB\"> 前年</span><span style=\"background-color: #FFF3F3\"> 同期</span><span style=\"background-color: #FFF6F6\"> 比</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFCFC\"> 円</span><span style=\"background-color: #FF8585\"> 減</span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFFAFA\"> [UNK]</span><span style=\"background-color: #FFF9F9\"> ．</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFCFC\"> ％</span><span style=\"background-color: #FF1313\"> 減</span><span style=\"background-color: #FFFDFD\"> ）</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFDFD\"> 営業</span><span style=\"background-color: #FF9393\"> 損失</span><span style=\"background-color: #FFFCFC\"> [UNK]</span><span style=\"background-color: #FFF7F7\"> 円</span><span style=\"background-color: #FFFEFE\"> （</span><span style=\"background-color: #FFFDFD\"> 前年</span><span style=\"background-color: #FFF8F8\"> 同期</span><span style=\"background-color: #FFF9F9\"> 比</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFFDFD\"> 円</span><span style=\"background-color: #FF0000\"> 減</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFDFD\"> 前年</span><span style=\"background-color: #FFEAEA\"> 同期</span><span style=\"background-color: #FFFDFD\"> 営業</span><span style=\"background-color: #FFF8F8\"> 利益</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFF9F9\"> 円</span><span style=\"background-color: #FFFEFE\"> ）</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FF9595\"> 損失</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFF7F7\"> 円</span><span style=\"background-color: #FFFEFE\"> （</span><span style=\"background-color: #FFFDFD\"> 前年</span><span style=\"background-color: #FFF8F8\"> 同期</span><span style=\"background-color: #FFFBFB\"> 比</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFFEFE\"> 円</span><span style=\"background-color: #FF6060\"> 減</span><span style=\"background-color: #FFFEFE\"> 、</span><span style=\"background-color: #FFFEFE\"> 前年</span><span style=\"background-color: #FFF2F2\"> 同期</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFF9F9\"> 利益</span><span style=\"background-color: #FFFEFE\"> [UNK]</span><span style=\"background-color: #FFFBFB\"> 円</span><span style=\"background-color: #FFFDFD\"> ）</span><span style=\"background-color: #FFF2F2\"> 、</span><span style=\"background-color: #FFF9F9\"> 親会社</span><span style=\"background-color: #FFF3F3\"> 株主</span><span style=\"background-color: #FFFDFD\"> に</span><span style=\"background-color: #FFFBFB\"> 帰属</span><span style=\"background-color: #FFFAFA\"> する</span><span style=\"background-color: #FFF8F8\"> [UNK]</span><span style=\"background-color: #FFF8F8\"> 純</span><span style=\"background-color: #FF5F5F\"> 損失</span><span style=\"background-color: #FFFAFA\"> [UNK]</span><span style=\"background-color: #FFEBEB\"> 円</span><span style=\"background-color: #FFFDFD\"> （</span><span style=\"background-color: #FFFCFC\"> 前年</span><span style=\"background-color: #FFF8F8\"> 同期</span><span style=\"background-color: #FFF9F9\"> 比</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFCFC\"> 円</span><span style=\"background-color: #FF5757\"> 減</span><span style=\"background-color: #FFFDFD\"> 、</span><span style=\"background-color: #FFFCFC\"> 前年</span><span style=\"background-color: #FFFCFC\"> 同期</span><span style=\"background-color: #FFF5F5\"> 親会社</span><span style=\"background-color: #FFFBFB\"> 株主</span><span style=\"background-color: #FFFEFE\"> に</span><span style=\"background-color: #FFFDFD\"> 帰属</span><span style=\"background-color: #FFFAFA\"> する</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFCFC\"> 純</span><span style=\"background-color: #FFFDFD\"> 利益</span><span style=\"background-color: #FFFDFD\"> [UNK]</span><span style=\"background-color: #FFFAFA\"> 円</span><span style=\"background-color: #FFFCFC\"> ）</span><span style=\"background-color: #FF9999\"> と</span><span style=\"background-color: #FFAFAF\"> なり</span><span style=\"background-color: #FFECEC\"> ました</span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"入力文章\")\n",
    "print(input_text)\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以上の結果、当連結会計年度における売上高1,785百万円(前年同期比357百万円減、16.7％減)、営業損失117百万円(前年同期比174百万円減、前年同期\\u3000営業利益57百万円)、経常損失112百万円(前年同期比183百万円減、前年同期\\u3000経常利益71百万円)、親会社株主に帰属する当期純損失58百万円(前年同期比116百万円減、前年同期\\u3000親会社株主に帰属する当期純利益57百万円)となりました'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids= tensor([[    2,   269,     5,   337,     6,   719,  3700,  5481,   594,     8,\n",
      "           217, 16720,     1,   176,     1,   387,    16,  2307,  3704,  2460,\n",
      "             1,   387,  4265,     6,     1,   264,     1,   257,  4265,    17,\n",
      "             6,   911,  7429,     1,   387,    16,  2307,  3704,  2460,     1,\n",
      "           387,  4265,     6,  2307,  3704,   911,  3718,     1,   387,    17,\n",
      "             6,     1,  7429,     1,   387,    16,  2307,  3704,  2460,     1,\n",
      "           387,  4265,     6,  2307,  3704,     1,  3718,     1,   387,    17,\n",
      "             6, 11100,  6970,     8, 13937,    22,     1,  3962,  7429,     1,\n",
      "           387,    16,  2307,  3704,  2460,     1,   387,  4265,     6,  2307,\n",
      "          3704, 11100,  6970,     8, 13937,    22,     1,  3962,  3718,     1,\n",
      "           387,    17,    12,   105,  4561,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "token_type_ids= None\n",
      "attention_mask= None\n",
      "output_all_encoded_layers= False\n",
      "attention_show_flg= True\n"
     ]
    }
   ],
   "source": [
    "outputs, attention_probs = net_trained(input, token_type_ids=None, attention_mask=None,\n",
    "                                       output_all_encoded_layers=False, attention_show_flg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
